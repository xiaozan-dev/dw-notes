

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>提交 Spark 应用程序 &mdash; Spark 2.2.x 中文文档 2.2.1 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="Spark 独立模式" href="spark-standalone.html" />
    <link rel="prev" title="集群模式概述" href="cluster-overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Spark 2.2.x 中文文档
          

          
          </a>

          
            
            
              <div class="version">
                2.2.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/quick-start.html">快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/rdd-guide.html">RDD 编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/sql-guide.html">Spark SQL, DataFrame 和 Dataset 编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/structured-streaming-guide.html">Structured Streaming编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/streaming-guide.html">Spark Streaming 编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/ml-guide.html">机器学习库(MLib)编程指南</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="cluster-overview.html">集群模式概述</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">提交 Spark 应用程序</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">应用程序依赖打包</a></li>
<li class="toctree-l2"><a class="reference internal" href="#spark-submit">使用 spark-submit 启动应用程序</a></li>
<li class="toctree-l2"><a class="reference internal" href="#master-urls">Master URLs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">从文件中加载配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">高级依赖管理</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">更多信息</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spark-standalone.html">Spark 独立模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="running-on-mesos.html">在Mesos上运行Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="running-on-yarn.html">在 YARN 上运行 Spark</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/configuration.html">Spark 配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/monitoring.html">监控和工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/tuning.html">Spark 性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/job-scheduling.html">Spark 任务调度</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/security.html">Spark安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/hardware-provisioning.html">硬件配置</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spark 2.2.x 中文文档</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>提交 Spark 应用程序</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/deploy-guide/submitting-applications.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark">
<span id="submitting-applications"></span><h1>提交 Spark 应用程序<a class="headerlink" href="#spark" title="永久链接至标题">¶</a></h1>
<p>Spark bin 目录下的 spark-submit 脚本用于在集群中启动 Spark 应用程序。通过一个统一的接口它可以使用 Spark 支持的所有类型的集群管理器, 因此不需要为每个集群管理器专门配置你的应用程序。</p>
<div class="section" id="id1">
<h2>应用程序依赖打包<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>如果你的代码依赖于其它工程，为了将代码发布到 Spark 集群, 你需要将应用程序的依赖项一起打包进来。这需要创建一个包含你自己代码和其依赖程序集 jar 包（或者 “uber” jar）。sbt 和 Maven 都有 assembly 插件。创建程序集 jar 包时，需要把 Spark 和 Hadoop 的 jar 包的依赖范围声明为 provided；因为这些 jar 包会由集群管理器在运行时提供，所以不需要再打包进来。一旦打完 jar 包之后，你就可以调用 bin/spark-submit 脚本来提交你的 jar 包了。</p>
<p>对于 Python，你可以使用 spark-submit 的 –py-files 参数，将你的程序以 .py、.zip 或 .egg 文件格式提交给集群。如果你需要依赖很多 Python 文件，我们推荐你将它们打成一个 .zip 或者 .egg 包。</p>
</div>
<div class="section" id="spark-submit">
<h2>使用 spark-submit 启动应用程序<a class="headerlink" href="#spark-submit" title="永久链接至标题">¶</a></h2>
<p>打包好一个应用程序之后，就可以使用 bin/spark-submit 脚本来提交它。这个脚本会负责设置 Spark 及其依赖的 classpath，同时它可以支持 Spark 所支持的所有不同类型的集群管理器和部署模式：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./bin/spark-submit <span class="se">\</span>
  --class &lt;main-class&gt;
  --master &lt;master-url&gt; <span class="se">\</span>
  --deploy-mode &lt;deploy-mode&gt; <span class="se">\</span>
  --conf &lt;key&gt;<span class="o">=</span>&lt;value&gt; <span class="se">\</span>
  ... <span class="c1"># 其他选项</span>
  &lt;application-jar&gt; <span class="se">\</span>
  <span class="o">[</span>application-arguments<span class="o">]</span>
</pre></div>
</div>
<p>一些常用选项如下：</p>
<ul class="simple">
<li><p>–class: 应用程序的入口（例如：org.apache.spark.examples.SparkPi）</p></li>
<li><p>–master: 集群的 master URL（如：spark://23.195.26.187:7077）</p></li>
<li><p>–deploy-mode: Driver 进程是在集群的 Worker 节点上运行（cluster模式），还是在本地作为一个外部客户端运行（client模式）（默认值是：client）</p></li>
<li><p>–conf: 可以设置任意的Spark配置属性，键值对（key=value）格式。如果值中包含空白字符，可以用双引号括起来（”key=value”）。</p></li>
<li><p>application-jar: 应用程序jar包路径，该jar包必须包括你自己的代码及其所有的依赖项。如果是URL，那么该路径URL必须是对整个集群可见且一致的，如：hdfs://path 或者 file://path （要求对所有节点都一致）</p></li>
<li><p>application-arguments: 传给应用程序入口类 main 函数的启动参数，可选。</p></li>
</ul>
<p>一种常见的部署策略是，从一台距离 Worker 节点的物理距离比较近的网关机器上提交你的应用程序(例如 Standalone EC2 集群中的 Master 节点)。这种情况下比较适合使用 client 模式。client 模式下，Driver 直接运行在 spark-submit 的进程中，对于集群来说它就像是一个客户端。应用程序的输入输出也被绑定到控制台上。因此，这种模式尤其适合于交互式执行（REPL）的应用程序，(例如 spark-shell)。</p>
<p>当然, 如果从距离 Worker 节点很远的机器(例如你的笔记本)上提交应用程序，通常使用 cluster 模式来尽量减少 Driver 和 Executor 之间的网络延迟。目前，只有 YARN 支持 Python 应用程序的 cluster 模式部署。</p>
<p>对于 Python 应用，只要把 &lt;application-jar&gt; 替换成一个 .py 文件，再把 Python 的 .zip、.egg 或者 .py 文件传给 –py-files 参数即可。</p>
<p>只有很少几个参数是专门用于所使用的集群管理器。例如，对于一个使用 cluster 模式部署的 Spark Standalone集群，你可以指定 –supervise 参数来确保 Driver 在异常退出码非0的情况下能够自动重启。运行 spark-submit –help 命令可查看所有这样的选项列表。下面是常用选项的几个示例：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 本地运行应用程序，使用8个core</span>
./bin/spark-submit <span class="se">\</span>
  --class org.apache.spark.examples.SparkPi <span class="se">\</span>
  --master local<span class="o">[</span><span class="m">8</span><span class="o">]</span> <span class="se">\</span>
  /path/to/examples.jar <span class="se">\</span>
  <span class="m">100</span>

<span class="c1"># 在 client 部署模式中的一个 Spark 独立集群上运行应用程序，</span>
./bin/spark-submit <span class="se">\</span>
  --class org.apache.spark.examples.SparkPi <span class="se">\</span>
  --master spark://207.184.161.138:7077 <span class="se">\</span>
  --executor-memory 20G <span class="se">\</span>
  --total-executor-cores <span class="m">100</span> <span class="se">\</span>
  /path/to/examples.jar <span class="se">\</span>
  <span class="m">1000</span>

<span class="c1"># 在 cluster 部署模式中的一个 Spark Standalone集群上运行应用程序，异常退出时自动重启</span>
./bin/spark-submit <span class="se">\</span>
  --class org.apache.spark.examples.SparkPi <span class="se">\</span>
  --master spark://207.184.161.138:7077 <span class="se">\</span>
  --deploy-mode cluster
  --supervise
  --executor-memory 20G <span class="se">\</span>
  --total-executor-cores <span class="m">100</span> <span class="se">\</span>
  /path/to/examples.jar <span class="se">\</span>
  <span class="m">1000</span>

<span class="c1"># 在 YARN 集群上运行应用程序</span>
<span class="nb">export</span> <span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>XXX
./bin/spark-submit <span class="se">\</span>
  --class org.apache.spark.examples.SparkPi <span class="se">\</span>
  --master yarn <span class="se">\</span>
  --deploy-mode cluster <span class="se">\ </span> <span class="c1"># 对于 client 模式其值为 client</span>
  --executor-memory 20G <span class="se">\</span>
  --num-executors <span class="m">50</span> <span class="se">\</span>
  /path/to/examples.jar <span class="se">\</span>
  <span class="m">1000</span>

<span class="c1"># 在一个 Spark Standalone集群上运行 python 应用程序</span>
./bin/spark-submit <span class="se">\</span>
  --master spark://207.184.161.138:7077 <span class="se">\</span>
  examples/src/main/python/pi.py <span class="se">\</span>
  <span class="m">1000</span>

<span class="c1"># 在 cluster 部署模式中的一个 Mesos 集群上运行应用程序，异常时自动重启</span>
./bin/spark-submit <span class="se">\</span>
  --class org.apache.spark.examples.SparkPi <span class="se">\</span>
  --master mesos://207.184.161.138:7077 <span class="se">\</span>
  --deploy-mode cluster
  --supervise
  --executor-memory 20G <span class="se">\</span>
  --total-executor-cores <span class="m">100</span> <span class="se">\</span>
  http://path/to/examples.jar <span class="se">\</span>
  <span class="m">1000</span>
</pre></div>
</div>
</div>
<div class="section" id="master-urls">
<h2>Master URLs<a class="headerlink" href="#master-urls" title="永久链接至标题">¶</a></h2>
<p>传给 Spark 的 master URL 可以是以下几种格式：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 8%" />
<col style="width: 92%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Master URL</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>local</p></td>
<td><p>本地运行 Spark，只使用 1 个 Worker 线程（即没有并行计算）</p></td>
</tr>
<tr class="row-odd"><td><p>local[K]</p></td>
<td><p>本地运行 Spark，使用 K 个 Worker 线程（理想情况是将这个值设为你机器上 CPU core的个数）</p></td>
</tr>
<tr class="row-even"><td><p>local[K,F]</p></td>
<td><p>本地运行 Spark，使用 K 个 Worker 线程并且允许 Task 最大失败次数为 F (see spark.task.maxFailures for an explanation of this variable)</p></td>
</tr>
<tr class="row-odd"><td><p>local[*]</p></td>
<td><p>本地运行 Spark，使用 Worker 线程数和机器上逻辑 CPU core个数一样。</p></td>
</tr>
<tr class="row-even"><td><p>local[<a href="#id2"><span class="problematic" id="id3">*</span></a>,F]</p></td>
<td><p>本地运行 Spark，使用 Worker 线程数和机器上逻辑 CPU core个数一样并且允许 Task 最大失败次数为 F。</p></td>
</tr>
<tr class="row-odd"><td><p>spark://HOST:PORT</p></td>
<td><p>连接到指定的 Spark Standalone 集群的 master。端口是可以配置的，默认是7077。</p></td>
</tr>
<tr class="row-even"><td><p>mesos://HOST:PORT</p></td>
<td><p>连接到指定的 Mesos 集群。端口号是可以配置的，默认是 5050。如果 Mesos 集群依赖于 ZooKeeper，可以使用 mesos://zk://… 来提交，注意 –deploy-mode需要设置为 cluster，同时，HOST:PORT 应指向 MesosClusterDispatcher.</p></td>
</tr>
<tr class="row-odd"><td><p>yarn</p></td>
<td><p>连接到指定的 YARN  集群，使用–deploy-mode 来指定 client 模式或是 cluster 模式。YARN 集群位置需要通过 $HADOOP_CONF_DIR 或者 $YARN_CONF_DIR 变量来查找。</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id4">
<h2>从文件中加载配置<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>spark-submit 脚本可以从一个属性文件加载默认的 Spark配置值，并将这些属性值传给你的应用程序。Spark 默认会从 Spark 安装目录中的 conf/spark-defaults.conf 文件读取这些属性配置。更详细信息，请参考 加载默认配置 这篇文章。</p>
<p>用这种方式加载默认Spark属性配置，可以在调用 spark-submit 脚本时省略一些参数标志。例如：如果属性文件中设置了 spark.master 属性，那么你就可以忽略 spark-submit 的 –master参数。通常，在代码里显示地在 SparkConf 对象上设置的参数具有最高的优先级，其次是 spark-submit 中传的参数，再次才是spark-defaults.conf文件中的配置值。</p>
<p>如果你总是搞不清楚最终生效的配置值是从哪里来的，你可以通过 spark-submit 的 —verbose 选项来打印细粒度的调试信息。</p>
</div>
<div class="section" id="id5">
<h2>高级依赖管理<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>使用 spark-submit 时，application jar 和 –jars 选项指定的 jar 包都会自动传到集群上。—jars后面的 URL 必须以逗号分隔。这个列表已经包含在驱动器和执行器的类路径上扩展目录不支持 —jars。</p>
<p>Spark 使用下面的URL协议以允许不同的 jar 包分发策略：</p>
<ul class="simple">
<li><p>file: – 文件绝对路径，并且file:/URI是通过驱动器的HTTP文件服务器来下载的，每个执行器都从驱动器的HTTP server拉取这些文件。</p></li>
<li><p>hdfs:, http:, https:, ftp: – 设置这些参数后，Spark将会从指定的URI位置下载所需的文件和jar包。</p></li>
<li><p>local: –  local:/ 打头的URI用于指定在每个工作节点上都能访问到的本地或共享文件。这意味着，不会占用网络IO，特别是对一些大文件或jar包，最好使用这种方式，当然，你需要把文件推送到每个工作节点上，或者通过NFS和GlusterFS共享文件。</p></li>
</ul>
<p>注意，每个 SparkContext 对应的 jar 包和文件都需要拷贝到所对应 Executor 的工作目录下。一段时间之后，这些文件可能会占用相当多的磁盘。在 YARN 上，这些清理工作是自动完成的；而在Spark独立部署时，这种自动清理需要配置 spark.worker.cleanup.appDataTtl 属性。</p>
<p>用户还可以用 –packages 参数，通过给定一个逗号分隔的 maven 坐标，来指定其它依赖项。这个命令会自动处理依赖树。额外的 maven库（或者SBT resolver）可以通过 –repositories 参数来指定。Spark 命令（pyspark，spark-shell，spark-submit）都支持这些参数。</p>
<p>对于 Python，也可以使用等价的 –py-files 选项来分发 .egg、.zip 以及 .py 库到执行器上。</p>
</div>
<div class="section" id="id6">
<h2>更多信息<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<p>部署完了你的应用程序后，集群模式概览 一文中描述了分布式执行中所涉及到的各个组件，以及如何监控和调试应用程序。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="spark-standalone.html" class="btn btn-neutral float-right" title="Spark 独立模式" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cluster-overview.html" class="btn btn-neutral float-left" title="集群模式概述" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, jackiehff

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>