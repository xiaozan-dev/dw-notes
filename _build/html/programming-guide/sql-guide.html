

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Spark SQL, DataFrame 和 Dataset 编程指南 &mdash; Spark 2.2.x 中文文档 2.2.1 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="Structured Streaming编程指南" href="structured-streaming-guide.html" />
    <link rel="prev" title="RDD 编程指南" href="rdd-guide.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Spark 2.2.x 中文文档
          

          
          </a>

          
            
            
              <div class="version">
                2.2.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick-start.html">快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="rdd-guide.html">RDD 编程指南</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Spark SQL, DataFrame 和 Dataset 编程指南</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">概述</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sql">SQL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-dataframe">Dataset 和 DataFrame</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">入门</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sparksession">入口: SparkSession</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataframe">创建 DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">无类型的 Dataset 操作 (亦即 DataFrame 操作)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-sql-queries-programmatically">Running SQL Queries Programmatically</a></li>
<li class="toctree-l3"><a class="reference internal" href="#global-temporary-view">Global Temporary View</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset">创建 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rdd">与 RDD 互操作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#programmatically-specifying-the-schema">Programmatically Specifying the Schema</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id4">聚合</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#untyped-user-defined-aggregate-functions">Untyped User-Defined Aggregate Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#type-safe-user-defined-aggregate-functions">Type-Safe User-Defined Aggregate Functions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id5">数据源</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#generic-load-save-functions">Generic Load/Save Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">手动指定选项</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">直接在文件上运行 SQL</a></li>
<li class="toctree-l4"><a class="reference internal" href="#save-modes">Save Modes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-to-persistent-tables">Saving to Persistent Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bucketing-sorting-and-partitioning">Bucketing, Sorting and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#parquet">Parquet 文件</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">编程方式加载数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">分区发现</a></li>
<li class="toctree-l4"><a class="reference internal" href="#schema">Schema 合并</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-parquet">Hive metastore Parquet表转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">配置</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#json-datasets">JSON Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hive-tables">Hive Tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#specifying-storage-format-for-hive-tables">Specifying storage format for Hive tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hive-metastore">与不同版本的Hive Metastore交互</a></li>
<li class="toctree-l3"><a class="reference internal" href="#jdbc-to-other-databases">JDBC To Other Databases</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id12">性能调优</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">缓存数据到内存中</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">其它配置选项</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id15">分布式 SQL 引擎</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#thrift-jdbc-odbc">运行 Thrift JDBC/ODBC 服务器</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-cli">运行 Spark SQL CLI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id16">迁移指南</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-2-1-2-2">Spark SQL 从 2.1 版本升级到2.2 版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-2-0-2-1">Spark SQL 从 2.0 版本升级到 2.1 版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-1-6-2-0">Spark SQL 从 1.6 版本升级到 2.0 版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-1-5-1-6">Spark SQL 从 1.5 版本升级到 1.6 版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-1-4-1-5">Spark SQL 从 1.4 版本升级到 1.5 版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-1-3-1-4">Spark SQL 从 1.3 版本升级到 1.4 版本</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">DataFrame数据读写接口</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dataframe-groupby">DataFrame.groupBy保留分组的列</a></li>
<li class="toctree-l4"><a class="reference internal" href="#behavior-change-on-dataframe-withcolumn">Behavior change on DataFrame.withColumn</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-sql-1-0-1-2-1-3">Spark SQL 从 1.0-1.2 版本升级到 1.3 版本</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#schemardddataframe">SchemaRDD重命名为DataFrame</a></li>
<li class="toctree-l4"><a class="reference internal" href="#javascala-api">统一Java和Scala API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dsl-scala">隔离隐式转换并删除dsl包(仅针对Scala)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#org-apache-spark-sqldatatype-scala">移除org.apache.spark.sql中DataType的类型别名(仅针对Scala)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#udfsqlcontext-udf-java-scala">UDF注册迁移到sqlContext.udf中(Java&amp;Scala)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pythondatatype">Python的DataType不再是单例的</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#apache-hive">兼容Apache Hive</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hive">在已有的Hive仓库中部署</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">支持的 Hive 功能</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">不支持的 Hive 功能</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id26">参考</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id27">数据类型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nan">NaN 语义</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="structured-streaming-guide.html">Structured Streaming编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="streaming-guide.html">Spark Streaming 编程指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-guide.html">机器学习库(MLib)编程指南</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guide/cluster-overview.html">集群模式概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guide/submitting-applications.html">提交 Spark 应用程序</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guide/spark-standalone.html">Spark 独立模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guide/running-on-mesos.html">在Mesos上运行Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guide/running-on-yarn.html">在 YARN 上运行 Spark</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/configuration.html">Spark 配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/monitoring.html">监控和工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/tuning.html">Spark 性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/job-scheduling.html">Spark 任务调度</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/security.html">Spark安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="../more-guide/hardware-provisioning.html">硬件配置</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spark 2.2.x 中文文档</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Spark SQL, DataFrame 和 Dataset 编程指南</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/programming-guide/sql-guide.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark-sql-dataframe-dataset">
<span id="sql-programming-guide"></span><h1>Spark SQL, DataFrame 和 Dataset 编程指南<a class="headerlink" href="#spark-sql-dataframe-dataset" title="永久链接至标题">¶</a></h1>
<div class="section" id="id1">
<h2>概述<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>Spark SQL 是 Spark 用于处理结构化数据的一个模块。不同于基础的 Spark RDD API，Spark SQL 提供的接口提供了更多关于数据和执行的计算任务的结构信息。Spark SQL 内部使用这些额外的信息来执行一些额外的优化操作。有几种方式可以与 Spark SQL 进
行交互，其中包括 SQL 和 Dataset API。当计算一个结果时 Spark SQL 使用的执行引擎是一样的, 它跟你使用哪种 API 或编程语
言来表达计算无关。这种统一意味着开发人员可以很容易地在不同的 API 之间来回切换，基于哪种 API 能够提供一种最自然的方式来表
达一个给定的变换（transformation）。</p>
<p>本文中所有的示例程序都使用 Spark 发行版本中自带的样本数据，并且可以在 spark-shell、pyspark shell 以及 sparkR shell 中运行。</p>
<div class="section" id="sql">
<h3>SQL<a class="headerlink" href="#sql" title="永久链接至标题">¶</a></h3>
<p>Spark SQL 的用法之一是执行 SQL 查询，它也可以从现有的 Hive 中读取数据，想要了解更多关于如何配置这个特性的细节, 请参考 Hive表 这节。
如果从其它编程语言内部运行 SQL，查询结果将作为一个 Dataset/DataFrame 返回。你还可以使用命令行或者通过 JDBC/ODBC 来
与 SQL 接口进行交互。</p>
</div>
<div class="section" id="dataset-dataframe">
<h3>Dataset 和 DataFrame<a class="headerlink" href="#dataset-dataframe" title="永久链接至标题">¶</a></h3>
<p>Dataset 是一个分布式数据集，它是 Spark 1.6 版本中新增的一个接口, 它结合了 RDD（强类型，可以使用强大的 lambda 表达式函数）
和 Spark SQL 的优化执行引擎的好处。Dataset 可以从 JVM 对象构造得到，随后可以使用函数式的变换（map，flatMap，filter 等）
进行操作。Dataset API 目前支持 Scala 和 Java 语言，还不支持 Python, 不过由于 Python 语言的动态性, Dataset API 的许
多好处早就已经可用了（例如，你可以使用 row.columnName 来访问数据行的某个字段）。这种场景对于 R 语言来说是类似的。</p>
<p>DataFrame 是按命名列方式组织的一个 Dataset。从概念上来讲，它等同于关系型数据库中的一张表或者 R 和 Python 中的一个 data frame，
只不过在底层进行了更多的优化。DataFrame 可以从很多数据源构造得到，比如：结构化的数据文件，Hive 表，外部数据库或现有的 RDD。
DataFrame API 支持 Scala, Java, Python 以及 R 语言。在 Scala 和 Java 语言中, DataFrame 由 Row 的 Dataset 来
表示的。在 Scala API 中, DataFrame 仅仅只是 Dataset[Row] 的一个类型别名，而在 Java API 中, 开发人员需要使用 Dataset&lt;Row&gt; 来
表示一个 DataFrame。</p>
<p>在本文档中, 我们将经常把 Scala/Java Row 的 Dataset 作为 DataFrame。</p>
</div>
</div>
<div class="section" id="id2">
<h2>入门<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<div class="section" id="sparksession">
<h3>入口: SparkSession<a class="headerlink" href="#sparksession" title="永久链接至标题">¶</a></h3>
<p><strong>Scala</strong></p>
<p>Spark 中所有功能的入口是 SparkSession 类。要创建一个基本的 SparkSession 对象, 只需要使用 SparkSession.builder():</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&quot;Spark SQL basic example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="s">&quot;spark.some.config.option&quot;</span><span class="o">,</span> <span class="s">&quot;some-value&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

<span class="c1">// For implicit conversions like converting RDDs to DataFrames</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>Spark 中所有功能的入口是 SparkSession 类。要创建一个基本的 SparkSession 对象, 只需要使用 SparkSession.builder():</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>

<span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
  <span class="o">.</span><span class="na">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="na">appName</span><span class="o">(</span><span class="s">&quot;Java Spark SQL basic example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">config</span><span class="o">(</span><span class="s">&quot;spark.some.config.option&quot;</span><span class="o">,</span> <span class="s">&quot;some-value&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">getOrCreate</span><span class="o">();</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>Spark 中所有功能的入口是 SparkSession 类。要创建一个基本的 SparkSession 对象, 只需要使用 SparkSession.builder:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL basic example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
<p><strong>R</strong></p>
<p>Spark 中所有功能的入口是 SparkSession 类。要初始化一个基本的 SparkSession 对象, 只需要调用 sparkR.session():</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="nf">sparkR.session</span><span class="p">(</span><span class="n">appName</span> <span class="o">=</span> <span class="s">&quot;R Spark SQL basic example&quot;</span><span class="p">,</span> <span class="n">sparkConfig</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">spark.some.config.option</span> <span class="o">=</span> <span class="s">&quot;some-value&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>当第一次调用时, sparkR.session() 会初始化一个全局的 SparkSession 单例实例, 并且总是为后续的调用返回该实例的引用。这样的话, 用户只需要初始化 SparkSession 一次, 然后像 read.df 这样的 SparkR 函数就可以隐式地访问该全局实例, 并且用户不需要传递 SparkSession 实例。</p>
</div>
<p>Spark 2.0 中的 SparkSession 提供了对 Hive 特性的内置支持，包括使用 HiveQL 编写查询，访问 Hive UDF 以及从 Hive 表读取数据。要使用这些特性，你不需要预先安装 Hive。</p>
</div>
<div class="section" id="dataframe">
<h3>创建 DataFrame<a class="headerlink" href="#dataframe" title="永久链接至标题">¶</a></h3>
<p><strong>Scala</strong></p>
<p>应用程序可以使用 SparkSession 从一个现有的 RDD，Hive 表或 Spark 数据源创建 DataFrame。</p>
<p>举个例子, 下面基于一个 JSON 文件的内容创建一个 DataFrame:</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">)</span>

<span class="c1">// Displays the content of the DataFrame to stdout</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>应用程序可以使用 SparkSession 从一个现有的 RDD，Hive 表或 Spark 数据源创建 DataFrame。</p>
<p>举个例子, 下面基于一个 JSON 文件的内容创建一个 DataFrame:</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">);</span>

<span class="c1">// Displays the content of the DataFrame to stdout</span>
<span class="n">df</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>应用程序可以使用 SparkSession 从一个现有的 RDD，Hive 表或 Spark 数据源创建 DataFrame。</p>
<p>举个例子, 下面基于一个 JSON 文件的内容创建一个 DataFrame:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>
<span class="c1"># Displays the content of the DataFrame to stdout</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># | age|   name|</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># |null|Michael|</span>
<span class="c1"># |  30|   Andy|</span>
<span class="c1"># |  19| Justin|</span>
<span class="c1"># +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
<p><strong>R</strong></p>
<p>应用程序可以使用 SparkSession 从一个本地的 R data.frame, Hive 表或 Spark 数据源创建 DataFrame。</p>
<p>举个例子, 下面基于一个 JSON 文件的内容创建一个 DataFrame:</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.json</span><span class="p">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># Displays the content of the DataFrame</span>
<span class="nf">head</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">##   age    name</span>
<span class="c1">## 1  NA Michael</span>
<span class="c1">## 2  30    Andy</span>
<span class="c1">## 3  19  Justin</span>

<span class="c1"># Another method to print the first few rows and optionally truncate the printing of long values</span>
<span class="nf">showDF</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">## +----+-------+</span>
<span class="c1">## | age|   name|</span>
<span class="c1">## +----+-------+</span>
<span class="c1">## |null|Michael|</span>
<span class="c1">## |  30|   Andy|</span>
<span class="c1">## |  19| Justin|</span>
<span class="c1">## +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="id3">
<h3>无类型的 Dataset 操作 (亦即 DataFrame 操作)<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>DataFrame 为 Scala, Java, Python 以及 R 语言中的结构化数据操作提供了一种领域特定语言。</p>
<p>正如上面所提到的,Spark 2.0 中, Scala 和 Java API 中的 DataFrame 只是 Row 的 Dataset。与使用强类型的 Scala/Java Dataset “强类型转换” 相比，这些操作也被称为 “非强类型转换” 。
These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.</p>
<p>下面是使用 Dataset 处理结构化数据的几个基础示例：</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// This import is needed to use the $-notation</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="c1">// Print the schema in a tree format</span>
<span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">// |-- age: long (nullable = true)</span>
<span class="c1">// |-- name: string (nullable = true)</span>

<span class="c1">// Select only the &quot;name&quot; column</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+</span>
<span class="c1">// |   name|</span>
<span class="c1">// +-------+</span>
<span class="c1">// |Michael|</span>
<span class="c1">// |   Andy|</span>
<span class="c1">// | Justin|</span>
<span class="c1">// +-------+</span>

<span class="c1">// Select everybody, but increment the age by 1</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="n">$</span><span class="s">&quot;age&quot;</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+---------+</span>
<span class="c1">// |   name|(age + 1)|</span>
<span class="c1">// +-------+---------+</span>
<span class="c1">// |Michael|     null|</span>
<span class="c1">// |   Andy|       31|</span>
<span class="c1">// | Justin|       20|</span>
<span class="c1">// +-------+---------+</span>

<span class="c1">// Select people older than 21</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;age&quot;</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+----+</span>
<span class="c1">// |age|name|</span>
<span class="c1">// +---+----+</span>
<span class="c1">// | 30|Andy|</span>
<span class="c1">// +---+----+</span>

<span class="c1">// Count people by age</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="n">count</span><span class="o">().</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-----+</span>
<span class="c1">// | age|count|</span>
<span class="c1">// +----+-----+</span>
<span class="c1">// |  19|    1|</span>
<span class="c1">// |null|    1|</span>
<span class="c1">// |  30|    1|</span>
<span class="c1">// +----+-----+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="c1">// col(&quot;...&quot;) is preferable to df.col(&quot;...&quot;)</span>
<span class="kn">import static</span> <span class="nn">org.apache.spark.sql.functions.col</span><span class="o">;</span>

<span class="c1">// Print the schema in a tree format</span>
<span class="n">df</span><span class="o">.</span><span class="na">printSchema</span><span class="o">();</span>
<span class="c1">// root</span>
<span class="c1">// |-- age: long (nullable = true)</span>
<span class="c1">// |-- name: string (nullable = true)</span>

<span class="c1">// Select only the &quot;name&quot; column</span>
<span class="n">df</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------+</span>
<span class="c1">// |   name|</span>
<span class="c1">// +-------+</span>
<span class="c1">// |Michael|</span>
<span class="c1">// |   Andy|</span>
<span class="c1">// | Justin|</span>
<span class="c1">// +-------+</span>

<span class="c1">// Select everybody, but increment the age by 1</span>
<span class="n">df</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">),</span> <span class="n">col</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="na">plus</span><span class="o">(</span><span class="mi">1</span><span class="o">)).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------+---------+</span>
<span class="c1">// |   name|(age + 1)|</span>
<span class="c1">// +-------+---------+</span>
<span class="c1">// |Michael|     null|</span>
<span class="c1">// |   Andy|       31|</span>
<span class="c1">// | Justin|       20|</span>
<span class="c1">// +-------+---------+</span>

<span class="c1">// Select people older than 21</span>
<span class="n">df</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="na">gt</span><span class="o">(</span><span class="mi">21</span><span class="o">)).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+----+</span>
<span class="c1">// |age|name|</span>
<span class="c1">// +---+----+</span>
<span class="c1">// | 30|Andy|</span>
<span class="c1">// +---+----+</span>

<span class="c1">// Count people by age</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="na">count</span><span class="o">().</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-----+</span>
<span class="c1">// | age|count|</span>
<span class="c1">// +----+-----+</span>
<span class="c1">// |  19|    1|</span>
<span class="c1">// |null|    1|</span>
<span class="c1">// |  30|    1|</span>
<span class="c1">// +----+-----+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>In Python it’s possible to access a DataFrame’s columns either by attribute (df.age) or by indexing (df[‘age’]). While the former is convenient for interactive data exploration, users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># spark, df are from the previous example</span>
<span class="c1"># Print the schema in a tree format</span>
<span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1"># root</span>
<span class="c1"># |-- age: long (nullable = true)</span>
<span class="c1"># |-- name: string (nullable = true)</span>

<span class="c1"># Select only the &quot;name&quot; column</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------+</span>
<span class="c1"># |   name|</span>
<span class="c1"># +-------+</span>
<span class="c1"># |Michael|</span>
<span class="c1"># |   Andy|</span>
<span class="c1"># | Justin|</span>
<span class="c1"># +-------+</span>

<span class="c1"># Select everybody, but increment the age by 1</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------+---------+</span>
<span class="c1"># |   name|(age + 1)|</span>
<span class="c1"># +-------+---------+</span>
<span class="c1"># |Michael|     null|</span>
<span class="c1"># |   Andy|       31|</span>
<span class="c1"># | Justin|       20|</span>
<span class="c1"># +-------+---------+</span>

<span class="c1"># Select people older than 21</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+----+</span>
<span class="c1"># |age|name|</span>
<span class="c1"># +---+----+</span>
<span class="c1"># | 30|Andy|</span>
<span class="c1"># +---+----+</span>

<span class="c1"># Count people by age</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +----+-----+</span>
<span class="c1"># | age|count|</span>
<span class="c1"># +----+-----+</span>
<span class="c1"># |  19|    1|</span>
<span class="c1"># |null|    1|</span>
<span class="c1"># |  30|    1|</span>
<span class="c1"># +----+-----+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the DataFrame</span>
<span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.json</span><span class="p">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># Show the content of the DataFrame</span>
<span class="nf">head</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">##   age    name</span>
<span class="c1">## 1  NA Michael</span>
<span class="c1">## 2  30    Andy</span>
<span class="c1">## 3  19  Justin</span>


<span class="c1"># Print the schema in a tree format</span>
<span class="nf">printSchema</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">## root</span>
<span class="c1">## |-- age: long (nullable = true)</span>
<span class="c1">## |-- name: string (nullable = true)</span>

<span class="c1"># Select only the &quot;name&quot; column</span>
<span class="nf">head</span><span class="p">(</span><span class="nf">select</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">))</span>
<span class="c1">##      name</span>
<span class="c1">## 1 Michael</span>
<span class="c1">## 2    Andy</span>
<span class="c1">## 3  Justin</span>

<span class="c1"># Select everybody, but increment the age by 1</span>
<span class="nf">head</span><span class="p">(</span><span class="nf">select</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">df</span><span class="o">$</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">$</span><span class="n">age</span> <span class="o">+</span> <span class="m">1</span><span class="p">))</span>
<span class="c1">##      name (age + 1.0)</span>
<span class="c1">## 1 Michael          NA</span>
<span class="c1">## 2    Andy          31</span>
<span class="c1">## 3  Justin          20</span>

<span class="c1"># Select people older than 21</span>
<span class="nf">head</span><span class="p">(</span><span class="nf">where</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">df</span><span class="o">$</span><span class="n">age</span> <span class="o">&gt;</span> <span class="m">21</span><span class="p">))</span>
<span class="c1">##   age name</span>
<span class="c1">## 1  30 Andy</span>

<span class="c1"># Count people by age</span>
<span class="nf">head</span><span class="p">(</span><span class="nf">count</span><span class="p">(</span><span class="nf">groupBy</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)))</span>
<span class="c1">##   age count</span>
<span class="c1">## 1  19     1</span>
<span class="c1">## 2  NA     1</span>
<span class="c1">## 3  30     1</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="running-sql-queries-programmatically">
<h3>Running SQL Queries Programmatically<a class="headerlink" href="#running-sql-queries-programmatically" title="永久链接至标题">¶</a></h3>
<p><strong>Scala</strong></p>
<p>The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Register the DataFrame as a SQL temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">sqlDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM people&quot;</span><span class="o">)</span>
<span class="n">sqlDF</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a Dataset&lt;Row&gt;.</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="c1">// Register the DataFrame as a SQL temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">);</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM people&quot;</span><span class="o">);</span>
<span class="n">sqlDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Register the DataFrame as a SQL temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM people&quot;</span><span class="p">)</span>
<span class="n">sqlDF</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># | age|   name|</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># |null|Michael|</span>
<span class="c1"># |  30|   Andy|</span>
<span class="c1"># |  19| Justin|</span>
<span class="c1"># +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
<p><strong>R</strong></p>
<p>The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">sql</span><span class="p">(</span><span class="s">&quot;SELECT * FROM table&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="global-temporary-view">
<h3>Global Temporary View<a class="headerlink" href="#global-temporary-view" title="永久链接至标题">¶</a></h3>
<p>Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1.</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Register the DataFrame as a global temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>

<span class="c1">// Global temporary view is tied to a system preserved database `global_temp`</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM global_temp.people&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>

<span class="c1">// Global temporary view is cross-session</span>
<span class="n">spark</span><span class="o">.</span><span class="n">newSession</span><span class="o">().</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM global_temp.people&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Register the DataFrame as a global temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="na">createGlobalTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">);</span>

<span class="c1">// Global temporary view is tied to a system preserved database `global_temp`</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM global_temp.people&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>

<span class="c1">// Global temporary view is cross-session</span>
<span class="n">spark</span><span class="o">.</span><span class="na">newSession</span><span class="o">().</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM global_temp.people&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Register the DataFrame as a global temporary view</span>
<span class="n">df</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># Global temporary view is tied to a system preserved database `global_temp`</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM global_temp.people&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># | age|   name|</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># |null|Michael|</span>
<span class="c1"># |  30|   Andy|</span>
<span class="c1"># |  19| Justin|</span>
<span class="c1"># +----+-------+</span>

<span class="c1"># Global temporary view is cross-session</span>
<span class="n">spark</span><span class="o">.</span><span class="n">newSession</span><span class="p">()</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM global_temp.people&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># | age|   name|</span>
<span class="c1"># +----+-------+</span>
<span class="c1"># |null|Michael|</span>
<span class="c1"># |  30|   Andy|</span>
<span class="c1"># |  19| Justin|</span>
<span class="c1"># +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">GLOBAL</span> <span class="k">TEMPORARY</span> <span class="k">VIEW</span> <span class="n">temp_view</span> <span class="k">AS</span> <span class="k">SELECT</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">b</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">FROM</span> <span class="n">tbl</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">global_temp</span><span class="p">.</span><span class="n">temp_view</span>
</pre></div>
</div>
</div>
<div class="section" id="dataset">
<h3>创建 Dataset<a class="headerlink" href="#dataset" title="永久链接至标题">¶</a></h3>
<p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="c1">// you can use custom classes that implement the Product interface</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">Person</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">age</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>

<span class="c1">// Encoders are created for case classes</span>
<span class="k">val</span> <span class="n">caseClassDS</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="nc">Person</span><span class="o">(</span><span class="s">&quot;Andy&quot;</span><span class="o">,</span> <span class="mi">32</span><span class="o">)).</span><span class="n">toDS</span><span class="o">()</span>
<span class="n">caseClassDS</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+---+</span>
<span class="c1">// |name|age|</span>
<span class="c1">// +----+---+</span>
<span class="c1">// |Andy| 32|</span>
<span class="c1">// +----+---+</span>

<span class="c1">// Encoders for most common types are automatically provided by importing spark.implicits._</span>
<span class="k">val</span> <span class="n">primitiveDS</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">).</span><span class="n">toDS</span><span class="o">()</span>
<span class="n">primitiveDS</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">collect</span><span class="o">()</span> <span class="c1">// Returns: Array(2, 3, 4)</span>

<span class="c1">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span>
<span class="k">val</span> <span class="n">path</span> <span class="k">=</span> <span class="s">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="k">val</span> <span class="n">peopleDS</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="n">path</span><span class="o">).</span><span class="n">as</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span>
<span class="n">peopleDS</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Collections</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoder</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Person</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">name</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">age</span><span class="o">;</span>

  <span class="kd">public</span> <span class="n">String</span> <span class="nf">getName</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">name</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setName</span><span class="o">(</span><span class="n">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getAge</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">age</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setAge</span><span class="o">(</span><span class="kt">int</span> <span class="n">age</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">age</span> <span class="o">=</span> <span class="n">age</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// Create an instance of a Bean class</span>
<span class="n">Person</span> <span class="n">person</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Person</span><span class="o">();</span>
<span class="n">person</span><span class="o">.</span><span class="na">setName</span><span class="o">(</span><span class="s">&quot;Andy&quot;</span><span class="o">);</span>
<span class="n">person</span><span class="o">.</span><span class="na">setAge</span><span class="o">(</span><span class="mi">32</span><span class="o">);</span>

<span class="c1">// Encoders are created for Java beans</span>
<span class="n">Encoder</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="n">personEncoder</span> <span class="o">=</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">bean</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="n">javaBeanDS</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataset</span><span class="o">(</span>
  <span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="n">person</span><span class="o">),</span>
  <span class="n">personEncoder</span>
<span class="o">);</span>
<span class="n">javaBeanDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+----+</span>
<span class="c1">// |age|name|</span>
<span class="c1">// +---+----+</span>
<span class="c1">// | 32|Andy|</span>
<span class="c1">// +---+----+</span>

<span class="c1">// Encoders for most common types are provided in class Encoders</span>
<span class="n">Encoder</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">integerEncoder</span> <span class="o">=</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">INT</span><span class="o">();</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">primitiveDS</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataset</span><span class="o">(</span><span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">),</span> <span class="n">integerEncoder</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">transformedDS</span> <span class="o">=</span> <span class="n">primitiveDS</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;)</span> <span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span>
    <span class="n">integerEncoder</span><span class="o">);</span>
<span class="n">transformedDS</span><span class="o">.</span><span class="na">collect</span><span class="o">();</span> <span class="c1">// Returns [2, 3, 4]</span>

<span class="c1">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span>
<span class="n">String</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">;</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="n">peopleDS</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="n">path</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="n">personEncoder</span><span class="o">);</span>
<span class="n">peopleDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// | age|   name|</span>
<span class="c1">// +----+-------+</span>
<span class="c1">// |null|Michael|</span>
<span class="c1">// |  30|   Andy|</span>
<span class="c1">// |  19| Justin|</span>
<span class="c1">// +----+-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
</div>
<div class="section" id="rdd">
<h3>与 RDD 互操作<a class="headerlink" href="#rdd" title="永久链接至标题">¶</a></h3>
<p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p>
<p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.</p>
<div class="section" id="inferring-the-schema-using-reflection">
<h4>Inferring the Schema Using Reflection<a class="headerlink" href="#inferring-the-schema-using-reflection" title="永久链接至标题">¶</a></h4>
<p><strong>Scala</strong></p>
<p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Seqs or Arrays. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements.</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// For implicit conversions from RDDs to DataFrames</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="c1">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span>
<span class="k">val</span> <span class="n">peopleDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
  <span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.txt&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">attributes</span> <span class="k">=&gt;</span> <span class="nc">Person</span><span class="o">(</span><span class="n">attributes</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">attributes</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">trim</span><span class="o">.</span><span class="n">toInt</span><span class="o">))</span>
  <span class="o">.</span><span class="n">toDF</span><span class="o">()</span>
<span class="c1">// Register the DataFrame as a temporary view</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>

<span class="c1">// SQL statements can be run by using the sql methods provided by Spark</span>
<span class="k">val</span> <span class="n">teenagersDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">)</span>

<span class="c1">// The columns of a row in the result can be accessed by field index</span>
<span class="n">teenagersDF</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">teenager</span> <span class="k">=&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">teenager</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>

<span class="c1">// or by field name</span>
<span class="n">teenagersDF</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">teenager</span> <span class="k">=&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">teenager</span><span class="o">.</span><span class="n">getAs</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&quot;name&quot;</span><span class="o">)).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>

<span class="c1">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span>
<span class="k">implicit</span> <span class="k">val</span> <span class="n">mapEncoder</span> <span class="k">=</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">Encoders</span><span class="o">.</span><span class="n">kryo</span><span class="o">[</span><span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Any</span><span class="o">]]</span>
<span class="c1">// Primitive types and case classes can be also defined as</span>
<span class="c1">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span>

<span class="c1">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
<span class="n">teenagersDF</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">teenager</span> <span class="k">=&gt;</span> <span class="n">teenager</span><span class="o">.</span><span class="n">getValuesMap</span><span class="o">[</span><span class="kt">Any</span><span class="o">](</span><span class="nc">List</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;age&quot;</span><span class="o">))).</span><span class="n">collect</span><span class="o">()</span>
<span class="c1">// Array(Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19))</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>Spark SQL supports automatically converting an RDD of JavaBeans into a DataFrame. The BeanInfo, obtained using reflection, defines the schema of the table. Currently, Spark SQL does not support JavaBeans that contain Map field(s). Nested JavaBeans and List or Array fields are supported though. You can create a JavaBean by creating a class that implements Serializable and has getters and setters for all of its fields.</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.Function</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoder</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>

<span class="c1">// Create an RDD of Person objects from a text file</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="n">peopleRDD</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.txt&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">javaRDD</span><span class="o">()</span>
  <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">line</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">String</span><span class="o">[]</span> <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">);</span>
    <span class="n">Person</span> <span class="n">person</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Person</span><span class="o">();</span>
    <span class="n">person</span><span class="o">.</span><span class="na">setName</span><span class="o">(</span><span class="n">parts</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span>
    <span class="n">person</span><span class="o">.</span><span class="na">setAge</span><span class="o">(</span><span class="n">Integer</span><span class="o">.</span><span class="na">parseInt</span><span class="o">(</span><span class="n">parts</span><span class="o">[</span><span class="mi">1</span><span class="o">].</span><span class="na">trim</span><span class="o">()));</span>
    <span class="k">return</span> <span class="n">person</span><span class="o">;</span>
  <span class="o">});</span>

<span class="c1">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">peopleDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">peopleRDD</span><span class="o">,</span> <span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="c1">// Register the DataFrame as a temporary view</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">);</span>

<span class="c1">// SQL statements can be run by using the sql methods provided by spark</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">teenagersDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">);</span>

<span class="c1">// The columns of a row in the result can be accessed by field index</span>
<span class="n">Encoder</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">stringEncoder</span> <span class="o">=</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">();</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">teenagerNamesByIndexDF</span> <span class="o">=</span> <span class="n">teenagersDF</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">row</span> <span class="o">-&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span>
    <span class="n">stringEncoder</span><span class="o">);</span>
<span class="n">teenagerNamesByIndexDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>

<span class="c1">// or by field name</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">teenagerNamesByFieldDF</span> <span class="o">=</span> <span class="n">teenagersDF</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">row</span> <span class="o">-&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.&lt;</span><span class="n">String</span><span class="o">&gt;</span><span class="n">getAs</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">),</span>
    <span class="n">stringEncoder</span><span class="o">);</span>
<span class="n">teenagerNamesByFieldDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># Load a text file and convert each line to a Row.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.txt&quot;</span><span class="p">)</span>
<span class="n">parts</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">age</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>

<span class="c1"># Infer the schema, and register the DataFrame as a table.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">)</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>

<span class="c1"># The results of SQL queries are Dataframe objects.</span>
<span class="c1"># rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.</span>
<span class="n">teenNames</span> <span class="o">=</span> <span class="n">teenagers</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">teenNames</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="c1"># Name: Justin</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
</div>
<div class="section" id="programmatically-specifying-the-schema">
<h4>Programmatically Specifying the Schema<a class="headerlink" href="#programmatically-specifying-the-schema" title="永久链接至标题">¶</a></h4>
<p><strong>Scala</strong></p>
<p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</p>
<p>Create an RDD of Rows from the original RDD;
Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.
For example:</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.types._</span>

<span class="c1">// Create an RDD</span>
<span class="k">val</span> <span class="n">peopleRDD</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.txt&quot;</span><span class="o">)</span>

<span class="c1">// The schema is encoded in a string</span>
<span class="k">val</span> <span class="n">schemaString</span> <span class="k">=</span> <span class="s">&quot;name age&quot;</span>

<span class="c1">// Generate the schema based on the string of schema</span>
<span class="k">val</span> <span class="n">fields</span> <span class="k">=</span> <span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">fieldName</span> <span class="k">=&gt;</span> <span class="nc">StructField</span><span class="o">(</span><span class="n">fieldName</span><span class="o">,</span> <span class="nc">StringType</span><span class="o">,</span> <span class="n">nullable</span> <span class="k">=</span> <span class="kc">true</span><span class="o">))</span>
<span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="nc">StructType</span><span class="o">(</span><span class="n">fields</span><span class="o">)</span>

<span class="c1">// Convert records of the RDD (people) to Rows</span>
<span class="k">val</span> <span class="n">rowRDD</span> <span class="k">=</span> <span class="n">peopleRDD</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">attributes</span> <span class="k">=&gt;</span> <span class="nc">Row</span><span class="o">(</span><span class="n">attributes</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">attributes</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">trim</span><span class="o">))</span>

<span class="c1">// Apply the schema to the RDD</span>
<span class="k">val</span> <span class="n">peopleDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">schema</span><span class="o">)</span>

<span class="c1">// Creates a temporary view using the DataFrame</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>

<span class="c1">// SQL can be run over a temporary view created using DataFrames</span>
<span class="k">val</span> <span class="n">results</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM people&quot;</span><span class="o">)</span>

<span class="c1">// The results of SQL queries are DataFrames and support all the normal RDD operations</span>
<span class="c1">// The columns of a row in the result can be accessed by field index or by field name</span>
<span class="n">results</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">attributes</span> <span class="k">=&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">attributes</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |        value|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |Name: Michael|</span>
<span class="c1">// |   Name: Andy|</span>
<span class="c1">// | Name: Justin|</span>
<span class="c1">// +-------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>When JavaBean classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a Dataset&lt;Row&gt; can be created programmatically with three steps.</p>
<p>Create an RDD of Rows from the original RDD;
Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.
For example:</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.Function</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.DataTypes</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.StructField</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.StructType</span><span class="o">;</span>

<span class="c1">// Create an RDD</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">peopleRDD</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sparkContext</span><span class="o">()</span>
  <span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.txt&quot;</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="na">toJavaRDD</span><span class="o">();</span>

<span class="c1">// The schema is encoded in a string</span>
<span class="n">String</span> <span class="n">schemaString</span> <span class="o">=</span> <span class="s">&quot;name age&quot;</span><span class="o">;</span>

<span class="c1">// Generate the schema based on the string of schema</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">StructField</span><span class="o">&gt;</span> <span class="n">fields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">fieldName</span> <span class="o">:</span> <span class="n">schemaString</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span> <span class="o">{</span>
  <span class="n">StructField</span> <span class="n">field</span> <span class="o">=</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructField</span><span class="o">(</span><span class="n">fieldName</span><span class="o">,</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">StringType</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
  <span class="n">fields</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">field</span><span class="o">);</span>
<span class="o">}</span>
<span class="n">StructType</span> <span class="n">schema</span> <span class="o">=</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructType</span><span class="o">(</span><span class="n">fields</span><span class="o">);</span>

<span class="c1">// Convert records of the RDD (people) to Rows</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">rowRDD</span> <span class="o">=</span> <span class="n">peopleRDD</span><span class="o">.</span><span class="na">map</span><span class="o">((</span><span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Row</span><span class="o">&gt;)</span> <span class="n">record</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">String</span><span class="o">[]</span> <span class="n">attributes</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">);</span>
  <span class="k">return</span> <span class="n">RowFactory</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">attributes</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">attributes</span><span class="o">[</span><span class="mi">1</span><span class="o">].</span><span class="na">trim</span><span class="o">());</span>
<span class="o">});</span>

<span class="c1">// Apply the schema to the RDD</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">peopleDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">schema</span><span class="o">);</span>

<span class="c1">// Creates a temporary view using the DataFrame</span>
<span class="n">peopleDataFrame</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">);</span>

<span class="c1">// SQL can be run over a temporary view created using DataFrames</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM people&quot;</span><span class="o">);</span>

<span class="c1">// The results of SQL queries are DataFrames and support all the normal RDD operations</span>
<span class="c1">// The columns of a row in the result can be accessed by field index or by field name</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">namesDS</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">row</span> <span class="o">-&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span>
    <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
<span class="n">namesDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |        value|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |Name: Michael|</span>
<span class="c1">// |   Name: Andy|</span>
<span class="c1">// | Name: Justin|</span>
<span class="c1">// +-------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>When a dictionary of kwargs cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</p>
<p>Create an RDD of tuples or lists from the original RDD;
Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.
Apply the schema to the RDD via createDataFrame method provided by SparkSession.
For example:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import data types</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># Load a text file and convert each line to a Row.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.txt&quot;</span><span class="p">)</span>
<span class="n">parts</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
<span class="c1"># Each line is converted to a tuple.</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>

<span class="c1"># The schema is encoded in a string.</span>
<span class="n">schemaString</span> <span class="o">=</span> <span class="s2">&quot;name age&quot;</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span><span class="n">StructField</span><span class="p">(</span><span class="n">field_name</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">field_name</span> <span class="ow">in</span> <span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span>

<span class="c1"># Apply the schema to the RDD.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="c1"># Creates a temporary view using the DataFrame</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people&quot;</span><span class="p">)</span>

<span class="n">results</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------+</span>
<span class="c1"># |   name|</span>
<span class="c1"># +-------+</span>
<span class="c1"># |Michael|</span>
<span class="c1"># |   Andy|</span>
<span class="c1"># | Justin|</span>
<span class="c1"># +-------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/basic.py” 文件。</p>
</div>
</div>
<div class="section" id="id4">
<h3>聚合<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>The built-in DataFrames functions provide common aggregations such as count(), countDistinct(), avg(), max(), min(), etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in Scala and Java to work with strongly typed Datasets. Moreover, users are not limited to the predefined aggregate functions and can create their own.</p>
<div class="section" id="untyped-user-defined-aggregate-functions">
<h4>Untyped User-Defined Aggregate Functions<a class="headerlink" href="#untyped-user-defined-aggregate-functions" title="永久链接至标题">¶</a></h4>
<p>Users have to extend the UserDefinedAggregateFunction abstract class to implement a custom untyped aggregate function. For example, a user-defined average can look like:</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.expressions.MutableAggregationBuffer</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.expressions.UserDefinedAggregateFunction</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.types._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.Row</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">object</span> <span class="nc">MyAverage</span> <span class="k">extends</span> <span class="nc">UserDefinedAggregateFunction</span> <span class="o">{</span>
  <span class="c1">// Data types of input arguments of this aggregate function</span>
  <span class="k">def</span> <span class="n">inputSchema</span><span class="k">:</span> <span class="kt">StructType</span> <span class="o">=</span> <span class="nc">StructType</span><span class="o">(</span><span class="nc">StructField</span><span class="o">(</span><span class="s">&quot;inputColumn&quot;</span><span class="o">,</span> <span class="nc">LongType</span><span class="o">)</span> <span class="o">::</span> <span class="nc">Nil</span><span class="o">)</span>
  <span class="c1">// Data types of values in the aggregation buffer</span>
  <span class="k">def</span> <span class="n">bufferSchema</span><span class="k">:</span> <span class="kt">StructType</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">StructType</span><span class="o">(</span><span class="nc">StructField</span><span class="o">(</span><span class="s">&quot;sum&quot;</span><span class="o">,</span> <span class="nc">LongType</span><span class="o">)</span> <span class="o">::</span> <span class="nc">StructField</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="nc">LongType</span><span class="o">)</span> <span class="o">::</span> <span class="nc">Nil</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// The data type of the returned value</span>
  <span class="k">def</span> <span class="n">dataType</span><span class="k">:</span> <span class="kt">DataType</span> <span class="o">=</span> <span class="nc">DoubleType</span>
  <span class="c1">// Whether this function always returns the same output on the identical input</span>
  <span class="k">def</span> <span class="n">deterministic</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">true</span>
  <span class="c1">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span>
  <span class="c1">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span>
  <span class="c1">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span>
  <span class="c1">// immutable.</span>
  <span class="k">def</span> <span class="n">initialize</span><span class="o">(</span><span class="n">buffer</span><span class="k">:</span> <span class="kt">MutableAggregationBuffer</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">buffer</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="k">=</span> <span class="mi">0L</span>
    <span class="n">buffer</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="k">=</span> <span class="mi">0L</span>
  <span class="o">}</span>
  <span class="c1">// Updates the given aggregation buffer `buffer` with new input data from `input`</span>
  <span class="k">def</span> <span class="n">update</span><span class="o">(</span><span class="n">buffer</span><span class="k">:</span> <span class="kt">MutableAggregationBuffer</span><span class="o">,</span> <span class="n">input</span><span class="k">:</span> <span class="kt">Row</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(!</span><span class="n">input</span><span class="o">.</span><span class="n">isNullAt</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
      <span class="n">buffer</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="k">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="n">input</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
      <span class="n">buffer</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="k">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
  <span class="o">}</span>
  <span class="c1">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span>
  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">buffer1</span><span class="k">:</span> <span class="kt">MutableAggregationBuffer</span><span class="o">,</span> <span class="n">buffer2</span><span class="k">:</span> <span class="kt">Row</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">buffer1</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="k">=</span> <span class="n">buffer1</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="n">buffer2</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
    <span class="n">buffer1</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="k">=</span> <span class="n">buffer1</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="n">buffer2</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// Calculates the final result</span>
  <span class="k">def</span> <span class="n">evaluate</span><span class="o">(</span><span class="n">buffer</span><span class="k">:</span> <span class="kt">Row</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">toDouble</span> <span class="o">/</span> <span class="n">buffer</span><span class="o">.</span><span class="n">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>

<span class="c1">// Register the function to access it</span>
<span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="s">&quot;myAverage&quot;</span><span class="o">,</span> <span class="nc">MyAverage</span><span class="o">)</span>

<span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/employees.json&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;employees&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |   name|salary|</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |Michael|  3000|</span>
<span class="c1">// |   Andy|  4500|</span>
<span class="c1">// | Justin|  3500|</span>
<span class="c1">// |  Berta|  4000|</span>
<span class="c1">// +-------+------+</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;</span><span class="o">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |average_salary|</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |        3750.0|</span>
<span class="c1">// +--------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.expressions.MutableAggregationBuffer</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.expressions.UserDefinedAggregateFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.DataType</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.DataTypes</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.StructField</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.types.StructType</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MyAverage</span> <span class="kd">extends</span> <span class="n">UserDefinedAggregateFunction</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="n">StructType</span> <span class="n">inputSchema</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">StructType</span> <span class="n">bufferSchema</span><span class="o">;</span>

  <span class="kd">public</span> <span class="nf">MyAverage</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">List</span><span class="o">&lt;</span><span class="n">StructField</span><span class="o">&gt;</span> <span class="n">inputFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
    <span class="n">inputFields</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructField</span><span class="o">(</span><span class="s">&quot;inputColumn&quot;</span><span class="o">,</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">LongType</span><span class="o">,</span> <span class="kc">true</span><span class="o">));</span>
    <span class="n">inputSchema</span> <span class="o">=</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructType</span><span class="o">(</span><span class="n">inputFields</span><span class="o">);</span>

    <span class="n">List</span><span class="o">&lt;</span><span class="n">StructField</span><span class="o">&gt;</span> <span class="n">bufferFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
    <span class="n">bufferFields</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructField</span><span class="o">(</span><span class="s">&quot;sum&quot;</span><span class="o">,</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">LongType</span><span class="o">,</span> <span class="kc">true</span><span class="o">));</span>
    <span class="n">bufferFields</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructField</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">LongType</span><span class="o">,</span> <span class="kc">true</span><span class="o">));</span>
    <span class="n">bufferSchema</span> <span class="o">=</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">createStructType</span><span class="o">(</span><span class="n">bufferFields</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="c1">// Data types of input arguments of this aggregate function</span>
  <span class="kd">public</span> <span class="n">StructType</span> <span class="nf">inputSchema</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">inputSchema</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// Data types of values in the aggregation buffer</span>
  <span class="kd">public</span> <span class="n">StructType</span> <span class="nf">bufferSchema</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">bufferSchema</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// The data type of the returned value</span>
  <span class="kd">public</span> <span class="n">DataType</span> <span class="nf">dataType</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">DoubleType</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// Whether this function always returns the same output on the identical input</span>
  <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">deterministic</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span>
  <span class="c1">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span>
  <span class="c1">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span>
  <span class="c1">// immutable.</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">initialize</span><span class="o">(</span><span class="n">MutableAggregationBuffer</span> <span class="n">buffer</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">buffer</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="n">L</span><span class="o">);</span>
    <span class="n">buffer</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">0</span><span class="n">L</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="c1">// Updates the given aggregation buffer `buffer` with new input data from `input`</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">update</span><span class="o">(</span><span class="n">MutableAggregationBuffer</span> <span class="n">buffer</span><span class="o">,</span> <span class="n">Row</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(!</span><span class="n">input</span><span class="o">.</span><span class="na">isNullAt</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
      <span class="kt">long</span> <span class="n">updatedSum</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="n">input</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
      <span class="kt">long</span> <span class="n">updatedCount</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">;</span>
      <span class="n">buffer</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">updatedSum</span><span class="o">);</span>
      <span class="n">buffer</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">updatedCount</span><span class="o">);</span>
    <span class="o">}</span>
  <span class="o">}</span>
  <span class="c1">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">merge</span><span class="o">(</span><span class="n">MutableAggregationBuffer</span> <span class="n">buffer1</span><span class="o">,</span> <span class="n">Row</span> <span class="n">buffer2</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">long</span> <span class="n">mergedSum</span> <span class="o">=</span> <span class="n">buffer1</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="n">buffer2</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
    <span class="kt">long</span> <span class="n">mergedCount</span> <span class="o">=</span> <span class="n">buffer1</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="n">buffer2</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
    <span class="n">buffer1</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">mergedSum</span><span class="o">);</span>
    <span class="n">buffer1</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">mergedCount</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="c1">// Calculates the final result</span>
  <span class="kd">public</span> <span class="n">Double</span> <span class="nf">evaluate</span><span class="o">(</span><span class="n">Row</span> <span class="n">buffer</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">((</span><span class="kt">double</span><span class="o">)</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">/</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// Register the function to access it</span>
<span class="n">spark</span><span class="o">.</span><span class="na">udf</span><span class="o">().</span><span class="na">register</span><span class="o">(</span><span class="s">&quot;myAverage&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="n">MyAverage</span><span class="o">());</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/employees.json&quot;</span><span class="o">);</span>
<span class="n">df</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;employees&quot;</span><span class="o">);</span>
<span class="n">df</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |   name|salary|</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |Michael|  3000|</span>
<span class="c1">// |   Andy|  4500|</span>
<span class="c1">// | Justin|  3500|</span>
<span class="c1">// |  Berta|  4000|</span>
<span class="c1">// +-------+------+</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;</span><span class="o">);</span>
<span class="n">result</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |average_salary|</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |        3750.0|</span>
<span class="c1">// +--------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java” 文件。</p>
</div>
<div class="section" id="type-safe-user-defined-aggregate-functions">
<h4>Type-Safe User-Defined Aggregate Functions<a class="headerlink" href="#type-safe-user-defined-aggregate-functions" title="永久链接至标题">¶</a></h4>
<p>User-defined aggregations for strongly typed Datasets revolve around the Aggregator abstract class. For example, a type-safe user-defined average can look like:</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.expressions.Aggregator</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.Encoder</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.Encoders</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Employee</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">salary</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">Average</span><span class="o">(</span><span class="k">var</span> <span class="n">sum</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>

<span class="k">object</span> <span class="nc">MyAverage</span> <span class="k">extends</span> <span class="nc">Aggregator</span><span class="o">[</span><span class="kt">Employee</span>, <span class="kt">Average</span>, <span class="kt">Double</span><span class="o">]</span> <span class="o">{</span>
  <span class="c1">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span>
  <span class="k">def</span> <span class="n">zero</span><span class="k">:</span> <span class="kt">Average</span> <span class="o">=</span> <span class="nc">Average</span><span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>
  <span class="c1">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span>
  <span class="c1">// and return it instead of constructing a new object</span>
  <span class="k">def</span> <span class="n">reduce</span><span class="o">(</span><span class="n">buffer</span><span class="k">:</span> <span class="kt">Average</span><span class="o">,</span> <span class="n">employee</span><span class="k">:</span> <span class="kt">Employee</span><span class="o">)</span><span class="k">:</span> <span class="kt">Average</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">buffer</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">employee</span><span class="o">.</span><span class="n">salary</span>
    <span class="n">buffer</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">buffer</span>
  <span class="o">}</span>
  <span class="c1">// Merge two intermediate values</span>
  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">b1</span><span class="k">:</span> <span class="kt">Average</span><span class="o">,</span> <span class="n">b2</span><span class="k">:</span> <span class="kt">Average</span><span class="o">)</span><span class="k">:</span> <span class="kt">Average</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">b1</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">b2</span><span class="o">.</span><span class="n">sum</span>
    <span class="n">b1</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">b2</span><span class="o">.</span><span class="n">count</span>
    <span class="n">b1</span>
  <span class="o">}</span>
  <span class="c1">// Transform the output of the reduction</span>
  <span class="k">def</span> <span class="n">finish</span><span class="o">(</span><span class="n">reduction</span><span class="k">:</span> <span class="kt">Average</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="n">reduction</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">toDouble</span> <span class="o">/</span> <span class="n">reduction</span><span class="o">.</span><span class="n">count</span>
  <span class="c1">// Specifies the Encoder for the intermediate value type</span>
  <span class="k">def</span> <span class="n">bufferEncoder</span><span class="k">:</span> <span class="kt">Encoder</span><span class="o">[</span><span class="kt">Average</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Encoders</span><span class="o">.</span><span class="n">product</span>
  <span class="c1">// Specifies the Encoder for the final output value type</span>
  <span class="k">def</span> <span class="n">outputEncoder</span><span class="k">:</span> <span class="kt">Encoder</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Encoders</span><span class="o">.</span><span class="n">scalaDouble</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">ds</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/employees.json&quot;</span><span class="o">).</span><span class="n">as</span><span class="o">[</span><span class="kt">Employee</span><span class="o">]</span>
<span class="n">ds</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |   name|salary|</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |Michael|  3000|</span>
<span class="c1">// |   Andy|  4500|</span>
<span class="c1">// | Justin|  3500|</span>
<span class="c1">// |  Berta|  4000|</span>
<span class="c1">// +-------+------+</span>

<span class="c1">// Convert the function to a `TypedColumn` and give it a name</span>
<span class="k">val</span> <span class="n">averageSalary</span> <span class="k">=</span> <span class="nc">MyAverage</span><span class="o">.</span><span class="n">toColumn</span><span class="o">.</span><span class="n">name</span><span class="o">(</span><span class="s">&quot;average_salary&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">averageSalary</span><span class="o">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |average_salary|</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |        3750.0|</span>
<span class="c1">// +--------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoder</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.TypedColumn</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.expressions.Aggregator</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Employee</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">name</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kt">long</span> <span class="n">salary</span><span class="o">;</span>

  <span class="c1">// Constructors, getters, setters...</span>

<span class="o">}</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Average</span> <span class="kd">implements</span> <span class="n">Serializable</span>  <span class="o">{</span>
  <span class="kd">private</span> <span class="kt">long</span> <span class="n">sum</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kt">long</span> <span class="n">count</span><span class="o">;</span>

  <span class="c1">// Constructors, getters, setters...</span>

<span class="o">}</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MyAverage</span> <span class="kd">extends</span> <span class="n">Aggregator</span><span class="o">&lt;</span><span class="n">Employee</span><span class="o">,</span> <span class="n">Average</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="c1">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span>
  <span class="kd">public</span> <span class="n">Average</span> <span class="nf">zero</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="k">new</span> <span class="n">Average</span><span class="o">(</span><span class="mi">0</span><span class="n">L</span><span class="o">,</span> <span class="mi">0</span><span class="n">L</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="c1">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span>
  <span class="c1">// and return it instead of constructing a new object</span>
  <span class="kd">public</span> <span class="n">Average</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Average</span> <span class="n">buffer</span><span class="o">,</span> <span class="n">Employee</span> <span class="n">employee</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">long</span> <span class="n">newSum</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getSum</span><span class="o">()</span> <span class="o">+</span> <span class="n">employee</span><span class="o">.</span><span class="na">getSalary</span><span class="o">();</span>
    <span class="kt">long</span> <span class="n">newCount</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="na">getCount</span><span class="o">()</span> <span class="o">+</span> <span class="mi">1</span><span class="o">;</span>
    <span class="n">buffer</span><span class="o">.</span><span class="na">setSum</span><span class="o">(</span><span class="n">newSum</span><span class="o">);</span>
    <span class="n">buffer</span><span class="o">.</span><span class="na">setCount</span><span class="o">(</span><span class="n">newCount</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">buffer</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// Merge two intermediate values</span>
  <span class="kd">public</span> <span class="n">Average</span> <span class="nf">merge</span><span class="o">(</span><span class="n">Average</span> <span class="n">b1</span><span class="o">,</span> <span class="n">Average</span> <span class="n">b2</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">long</span> <span class="n">mergedSum</span> <span class="o">=</span> <span class="n">b1</span><span class="o">.</span><span class="na">getSum</span><span class="o">()</span> <span class="o">+</span> <span class="n">b2</span><span class="o">.</span><span class="na">getSum</span><span class="o">();</span>
    <span class="kt">long</span> <span class="n">mergedCount</span> <span class="o">=</span> <span class="n">b1</span><span class="o">.</span><span class="na">getCount</span><span class="o">()</span> <span class="o">+</span> <span class="n">b2</span><span class="o">.</span><span class="na">getCount</span><span class="o">();</span>
    <span class="n">b1</span><span class="o">.</span><span class="na">setSum</span><span class="o">(</span><span class="n">mergedSum</span><span class="o">);</span>
    <span class="n">b1</span><span class="o">.</span><span class="na">setCount</span><span class="o">(</span><span class="n">mergedCount</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">b1</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="c1">// Transform the output of the reduction</span>
  <span class="kd">public</span> <span class="n">Double</span> <span class="nf">finish</span><span class="o">(</span><span class="n">Average</span> <span class="n">reduction</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">((</span><span class="kt">double</span><span class="o">)</span> <span class="n">reduction</span><span class="o">.</span><span class="na">getSum</span><span class="o">())</span> <span class="o">/</span> <span class="n">reduction</span><span class="o">.</span><span class="na">getCount</span><span class="o">();</span>
  <span class="o">}</span>
  <span class="c1">// Specifies the Encoder for the intermediate value type</span>
  <span class="kd">public</span> <span class="n">Encoder</span><span class="o">&lt;</span><span class="n">Average</span><span class="o">&gt;</span> <span class="nf">bufferEncoder</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">bean</span><span class="o">(</span><span class="n">Average</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="c1">// Specifies the Encoder for the final output value type</span>
  <span class="kd">public</span> <span class="n">Encoder</span><span class="o">&lt;</span><span class="n">Double</span><span class="o">&gt;</span> <span class="nf">outputEncoder</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">DOUBLE</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="n">Encoder</span><span class="o">&lt;</span><span class="n">Employee</span><span class="o">&gt;</span> <span class="n">employeeEncoder</span> <span class="o">=</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">bean</span><span class="o">(</span><span class="n">Employee</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">String</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&quot;examples/src/main/resources/employees.json&quot;</span><span class="o">;</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Employee</span><span class="o">&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="n">path</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="n">employeeEncoder</span><span class="o">);</span>
<span class="n">ds</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |   name|salary|</span>
<span class="c1">// +-------+------+</span>
<span class="c1">// |Michael|  3000|</span>
<span class="c1">// |   Andy|  4500|</span>
<span class="c1">// | Justin|  3500|</span>
<span class="c1">// |  Berta|  4000|</span>
<span class="c1">// +-------+------+</span>

<span class="n">MyAverage</span> <span class="n">myAverage</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MyAverage</span><span class="o">();</span>
<span class="c1">// Convert the function to a `TypedColumn` and give it a name</span>
<span class="n">TypedColumn</span><span class="o">&lt;</span><span class="n">Employee</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="n">averageSalary</span> <span class="o">=</span> <span class="n">myAverage</span><span class="o">.</span><span class="na">toColumn</span><span class="o">().</span><span class="na">name</span><span class="o">(</span><span class="s">&quot;average_salary&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Double</span><span class="o">&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="n">averageSalary</span><span class="o">);</span>
<span class="n">result</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |average_salary|</span>
<span class="c1">// +--------------+</span>
<span class="c1">// |        3750.0|</span>
<span class="c1">// +--------------+</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java” 文件。</p>
</div>
</div>
</div>
<div class="section" id="id5">
<h2>数据源<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.</p>
<div class="section" id="generic-load-save-functions">
<h3>Generic Load/Save Functions<a class="headerlink" href="#generic-load-save-functions" title="永久链接至标题">¶</a></h3>
<p>In the simplest form, the default data source (parquet unless otherwise configured by spark.sql.sources.default) will be used for all operations.</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">usersDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="o">)</span>
<span class="n">usersDF</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;favorite_color&quot;</span><span class="o">).</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="o">(</span><span class="s">&quot;namesAndFavColors.parquet&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">usersDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">load</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="o">);</span>
<span class="n">usersDF</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;favorite_color&quot;</span><span class="o">).</span><span class="na">write</span><span class="o">().</span><span class="na">save</span><span class="o">(</span><span class="s">&quot;namesAndFavColors.parquet&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite_color&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndFavColors.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.df</span><span class="p">(</span><span class="s">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="p">)</span>
<span class="nf">write.df</span><span class="p">(</span><span class="nf">select</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;favorite_color&quot;</span><span class="p">),</span> <span class="s">&quot;namesAndFavColors.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
<div class="section" id="id6">
<h4>手动指定选项<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h4>
<p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., org.apache.spark.sql.parquet), but for built-in sources you can also use their short names (json, parquet, jdbc, orc, libsvm, csv, text). DataFrames loaded from any data source type can be converted into other types using this syntax.</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">peopleDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;json&quot;</span><span class="o">).</span><span class="n">load</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">)</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;age&quot;</span><span class="o">).</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">).</span><span class="n">save</span><span class="o">(</span><span class="s">&quot;namesAndAges.parquet&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">peopleDF</span> <span class="o">=</span>
  <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;json&quot;</span><span class="o">).</span><span class="na">load</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">);</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;age&quot;</span><span class="o">).</span><span class="na">write</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">).</span><span class="na">save</span><span class="o">(</span><span class="s">&quot;namesAndAges.parquet&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndAges.parquet&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.df</span><span class="p">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="s">&quot;json&quot;</span><span class="p">)</span>
<span class="n">namesAndAges</span> <span class="o">&lt;-</span> <span class="nf">select</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="nf">write.df</span><span class="p">(</span><span class="n">namesAndAges</span><span class="p">,</span> <span class="s">&quot;namesAndAges.parquet&quot;</span><span class="p">,</span> <span class="s">&quot;parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="id7">
<h4>直接在文件上运行 SQL<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">sqlDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">sqlDF</span> <span class="o">=</span>
  <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">sql</span><span class="p">(</span><span class="s">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="save-modes">
<h4>Save Modes<a class="headerlink" href="#save-modes" title="永久链接至标题">¶</a></h4>
<p>Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 6%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Scala/Java</p></th>
<th class="head"><p>Any Language</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SaveMode.ErrorIfExists(default)</p></td>
<td><p>“error” (default)</p></td>
<td><p>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</p></td>
</tr>
<tr class="row-odd"><td><p>SaveMode.Append</p></td>
<td><p>“append”</p></td>
<td><p>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</p></td>
</tr>
<tr class="row-even"><td><p>SaveMode.Overwrite</p></td>
<td><p>“overwrite”</p></td>
<td><p>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</p></td>
</tr>
<tr class="row-odd"><td><p>SaveMode.Ignore</p></td>
<td><p>“ignore”</p></td>
<td><p>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="saving-to-persistent-tables">
<h4>Saving to Persistent Tables<a class="headerlink" href="#saving-to-persistent-tables" title="永久链接至标题">¶</a></h4>
<p>DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table.</p>
<p>For file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, e.g. df.write.option(“path”, “/some/path”).saveAsTable(“t”). When the table is dropped, the custom table path will not be removed and the table data is still there. If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too.</p>
<p>Starting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore. This brings several benefits:</p>
<p>Since the metastore can return only necessary partitions for a query, discovering all the partitions on the first query to the table is no longer needed.
Hive DDLs such as ALTER TABLE PARTITION … SET LOCATION are now available for tables created with the Datasource API.
Note that partition information is not gathered by default when creating external datasource tables (those with a path option). To sync the partition information in the metastore, you can invoke MSCK REPAIR TABLE.</p>
</div>
<div class="section" id="bucketing-sorting-and-partitioning">
<h4>Bucketing, Sorting and Partitioning<a class="headerlink" href="#bucketing-sorting-and-partitioning" title="永久链接至标题">¶</a></h4>
<p>For file-based data source, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables:</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="n">peopleDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">bucketBy</span><span class="o">(</span><span class="mi">42</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">).</span><span class="n">sortBy</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;people_bucketed&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p>while partitioning can be used with both save and saveAsTable when using the Dataset APIs.</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="n">usersDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="o">(</span><span class="s">&quot;favorite_color&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">).</span><span class="n">save</span><span class="o">(</span><span class="s">&quot;namesPartByColor.parquet&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。
It is possible to use both partitioning and bucketing for a single table:</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="n">peopleDF</span>
  <span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">partitionBy</span><span class="o">(</span><span class="s">&quot;favorite_color&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">bucketBy</span><span class="o">(</span><span class="mi">42</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;people_partitioned_bucketed&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。
partitionBy creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast bucketBy distributes data across a fixed number of buckets and can be used when a number of unique values is unbounded.</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">peopleDF</span><span class="o">.</span><span class="na">write</span><span class="o">().</span><span class="na">bucketBy</span><span class="o">(</span><span class="mi">42</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">).</span><span class="na">sortBy</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="na">saveAsTable</span><span class="o">(</span><span class="s">&quot;people_bucketed&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。
while partitioning can be used with both save and saveAsTable when using the Dataset APIs.</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">usersDF</span>
  <span class="o">.</span><span class="na">write</span><span class="o">()</span>
  <span class="o">.</span><span class="na">partitionBy</span><span class="o">(</span><span class="s">&quot;favorite_color&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">save</span><span class="o">(</span><span class="s">&quot;namesPartByColor.parquet&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。
It is possible to use both partitioning and bucketing for a single table:</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">peopleDF</span>
  <span class="o">.</span><span class="na">write</span><span class="o">()</span>
  <span class="o">.</span><span class="na">partitionBy</span><span class="o">(</span><span class="s">&quot;favorite_color&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">bucketBy</span><span class="o">(</span><span class="mi">42</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">saveAsTable</span><span class="o">(</span><span class="s">&quot;people_partitioned_bucketed&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。
partitionBy creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast bucketBy distributes data across a fixed number of buckets and can be used when a number of unique values is unbounded.</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sortBy</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;people_bucketed&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p>while partitioning can be used with both save and saveAsTable when using the Dataset APIs.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;favorite_color&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesPartByColor.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p>It is possible to use both partitioning and bucketing for a single table:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">df</span>
    <span class="o">.</span><span class="n">write</span>
    <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;favorite_color&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;people_partitioned_bucketed&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>完整的示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。
partitionBy creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast bucketBy distributes data across a fixed number of buckets and can be used when a number of unique values is unbounded.</p>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">users_bucketed_by_name</span><span class="p">(</span>
  <span class="n">name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_color</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_numbers</span> <span class="nb">array</span><span class="o">&lt;</span><span class="nb">integer</span><span class="o">&gt;</span>
<span class="p">)</span> <span class="k">USING</span> <span class="n">parquet</span>
<span class="n">CLUSTERED</span> <span class="k">BY</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">INTO</span> <span class="mi">42</span> <span class="n">BUCKETS</span><span class="p">;</span>
</pre></div>
</div>
<p>while partitioning can be used with both save and saveAsTable when using the Dataset APIs.</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">users_by_favorite_color</span><span class="p">(</span>
  <span class="n">name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_color</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_numbers</span> <span class="nb">array</span><span class="o">&lt;</span><span class="nb">integer</span><span class="o">&gt;</span>
<span class="p">)</span> <span class="k">USING</span> <span class="n">csv</span> <span class="n">PARTITIONED</span> <span class="k">BY</span><span class="p">(</span><span class="n">favorite_color</span><span class="p">);</span>
</pre></div>
</div>
<p>It is possible to use both partitioning and bucketing for a single table:</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">users_bucketed_and_partitioned</span><span class="p">(</span>
  <span class="n">name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_color</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">favorite_numbers</span> <span class="nb">array</span><span class="o">&lt;</span><span class="nb">integer</span><span class="o">&gt;</span>
<span class="p">)</span> <span class="k">USING</span> <span class="n">parquet</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">favorite_color</span><span class="p">)</span>
<span class="n">CLUSTERED</span> <span class="k">BY</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="n">SORTED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">favorite_numbers</span><span class="p">)</span> <span class="k">INTO</span> <span class="mi">42</span> <span class="n">BUCKETS</span><span class="p">;</span>
</pre></div>
</div>
<p>partitionBy creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast bucketBy distributes data across a fixed number of buckets and can be used when a number of unique values is unbounded.</p>
</div>
</div>
<div class="section" id="parquet">
<h3>Parquet 文件<a class="headerlink" href="#parquet" title="永久链接至标题">¶</a></h3>
<p>Parquet 是一种列式存储格式，很多其它的数据处理系统都支持它。Spark SQL 提供了对 Parquet 文件的读写支持，而且 Parquet 文件能够自动保存原始数据的 schema。写 Parquet 文件的时候，所有列都自动地转化成 nullable，以便向后兼容。</p>
<div class="section" id="id8">
<h4>编程方式加载数据<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h4>
<p>仍然使用上面例子中的数据：</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Encoders for most common types are automatically provided by importing spark.implicits._</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="k">val</span> <span class="n">peopleDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">)</span>

<span class="c1">// DataFrames can be saved as Parquet files, maintaining the schema information</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;people.parquet&quot;</span><span class="o">)</span>

<span class="c1">// Read in the parquet file created above</span>
<span class="c1">// Parquet files are self-describing so the schema is preserved</span>
<span class="c1">// The result of loading a Parquet file is also a DataFrame</span>
<span class="k">val</span> <span class="n">parquetFileDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;people.parquet&quot;</span><span class="o">)</span>

<span class="c1">// Parquet files can also be used to create a temporary view and then used in SQL statements</span>
<span class="n">parquetFileDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;parquetFile&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">namesDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">)</span>
<span class="n">namesDF</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">attributes</span> <span class="k">=&gt;</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">attributes</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>
<span class="c1">// import org.apache.spark.sql.Encoders;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">peopleDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">);</span>

<span class="c1">// DataFrames can be saved as Parquet files, maintaining the schema information</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="na">write</span><span class="o">().</span><span class="na">parquet</span><span class="o">(</span><span class="s">&quot;people.parquet&quot;</span><span class="o">);</span>

<span class="c1">// Read in the Parquet file created above.</span>
<span class="c1">// Parquet files are self-describing so the schema is preserved</span>
<span class="c1">// The result of loading a parquet file is also a DataFrame</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">parquetFileDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">parquet</span><span class="o">(</span><span class="s">&quot;people.parquet&quot;</span><span class="o">);</span>

<span class="c1">// Parquet files can also be used to create a temporary view and then used in SQL statements</span>
<span class="n">parquetFileDF</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;parquetFile&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">namesDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">namesDS</span> <span class="o">=</span> <span class="n">namesDF</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">call</span><span class="o">(</span><span class="n">Row</span> <span class="n">row</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="s">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">},</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
<span class="n">namesDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +------------+</span>
<span class="c1">// |       value|</span>
<span class="c1">// +------------+</span>
<span class="c1">// |Name: Justin|</span>
<span class="c1">// +------------+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">peopleDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># DataFrames can be saved as Parquet files, maintaining the schema information.</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Read in the Parquet file created above.</span>
<span class="c1"># Parquet files are self-describing so the schema is preserved.</span>
<span class="c1"># The result of loading a parquet file is also a DataFrame.</span>
<span class="n">parquetFile</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Parquet files can also be used to create a temporary view and then used in SQL statements.</span>
<span class="n">parquetFile</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;parquetFile&quot;</span><span class="p">)</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="n">teenagers</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +------+</span>
<span class="c1"># |  name|</span>
<span class="c1"># +------+</span>
<span class="c1"># |Justin|</span>
<span class="c1"># +------+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.df</span><span class="p">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="s">&quot;json&quot;</span><span class="p">)</span>

<span class="c1"># SparkDataFrame can be saved as Parquet files, maintaining the schema information.</span>
<span class="nf">write.parquet</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="c1"># The result of loading a parquet file is also a DataFrame.</span>
<span class="n">parquetFile</span> <span class="o">&lt;-</span> <span class="nf">read.parquet</span><span class="p">(</span><span class="s">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Parquet files can also be used to create a temporary view and then used in SQL statements.</span>
<span class="nf">createOrReplaceTempView</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">,</span> <span class="s">&quot;parquetFile&quot;</span><span class="p">)</span>
<span class="n">teenagers</span> <span class="o">&lt;-</span> <span class="nf">sql</span><span class="p">(</span><span class="s">&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">teenagers</span><span class="p">)</span>
<span class="c1">##     name</span>
<span class="c1">## 1 Justin</span>

<span class="c1"># We can also run custom R-UDFs on Spark DataFrames. Here we prefix all the names with &quot;Name:&quot;</span>
<span class="n">schema</span> <span class="o">&lt;-</span> <span class="nf">structType</span><span class="p">(</span><span class="nf">structField</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;string&quot;</span><span class="p">))</span>
<span class="n">teenNames</span> <span class="o">&lt;-</span> <span class="nf">dapply</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="nf">function</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">{</span> <span class="nf">cbind</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Name:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">$</span><span class="n">name</span><span class="p">))</span> <span class="p">},</span> <span class="n">schema</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">teenName</span> <span class="n">in</span> <span class="nf">collect</span><span class="p">(</span><span class="n">teenNames</span><span class="p">)</span><span class="o">$</span><span class="n">name</span><span class="p">)</span> <span class="p">{</span>
  <span class="nf">cat</span><span class="p">(</span><span class="n">teenName</span><span class="p">,</span> <span class="s">&quot;\n&quot;</span><span class="p">)</span>
<span class="p">}</span>
<span class="c1">## Name: Michael</span>
<span class="c1">## Name: Andy</span>
<span class="c1">## Name: Justin</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TEMPORARY</span> <span class="k">VIEW</span> <span class="n">parquetTable</span>
<span class="k">USING</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="k">sql</span><span class="p">.</span><span class="n">parquet</span>
<span class="k">OPTIONS</span> <span class="p">(</span>
  <span class="n">path</span> <span class="ss">&quot;examples/src/main/resources/people.parquet&quot;</span>
<span class="p">)</span>

<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">parquetTable</span>
</pre></div>
</div>
</div>
<div class="section" id="id9">
<h4>分区发现<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h4>
<p>像Hive这样的系统中，一个常用的优化方式就是表分区。在一个分区表中，数据通常存储在不同的目录中，分区列值被编码到各个分区目录的路径。Parquet数据源现在可以自动发现和推导分区信息。例如，我们可以使用下面的目录结构把之前使用的人口数据存储到一个分区表中，其中2个额外的字段，gender和country，作为分区列：</p>
<div class="highlight-TEXT notranslate"><div class="highlight"><pre><span></span>path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ...
</pre></div>
</div>
<p>通过传递 path/to/table 给 SparkSession.read.parquet 或 SparkSession.read.load, Spark SQL将会自动从路径中提取分区信息。现在返回的DataFrame的schema如下：</p>
<div class="highlight-TEXT notranslate"><div class="highlight"><pre><span></span>root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
</pre></div>
</div>
<p>注意，分区列的数据类型是自动推导出来的。目前，分区列只支持数值类型和字符串类型。有时候用户可能不想要自动推导分区列的数据类型，对于这种情况，自动类型推导可以通过 spark.sql.sources.partitionColumnTypeInference.enabled来配置，其默认值是true。当禁用类型推导后，字符串类型将用于分区列类型。</p>
<p>从Spark 1.6.0 版本开始，分区发现默认只查找给定路径下的分区。拿上面的例子来说，如果用户传递 path/to/table/gender=male 给 SparkSession.read.parquet 或者 SparkSession.read.load，那么gender将不会被当作分区列。如果用户想要指定分区发现开始的基础目录，可以在数据源选项中设置basePath。例如，如果把 path/to/table/gender=male作为数据目录，并且将basePath设为 path/to/table，那么gender仍然会最为分区键。</p>
</div>
<div class="section" id="schema">
<h4>Schema 合并<a class="headerlink" href="#schema" title="永久链接至标题">¶</a></h4>
<p>和 ProtocolBuffer、Avro 以及 Thrift 一样，Parquet也支持 schema 演变。用户可以从一个简单的 schema 开始，逐渐增加所需要的列。这样的话，用户最终会得到多个Parquet文件, 这些文件的schema不同但互相兼容。Parquet数据源目前已经支持自动检测这种情况并合并所有这些文件的schema。</p>
<p>因为schema合并相对来说是一个代价高昂的操作，并且在大多数情况下不需要，所以从Spark 1.5.0 版本开始，默认禁用Schema合并。你可以这样启用这一功能：</p>
<ol class="arabic simple">
<li><p>当读取Parquet文件时，将数据源选项 mergeSchema设置为true（见下面的示例代码）</p></li>
<li><p>或者，将全局SQL选项 spark.sql.parquet.mergeSchema设置为true。</p></li>
</ol>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// This is used to implicitly convert an RDD to a DataFrame.</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="c1">// Create a simple DataFrame, store into a partition directory</span>
<span class="k">val</span> <span class="n">squaresDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">5</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;value&quot;</span><span class="o">,</span> <span class="s">&quot;square&quot;</span><span class="o">)</span>
<span class="n">squaresDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table/key=1&quot;</span><span class="o">)</span>

<span class="c1">// Create another DataFrame in a new partition directory,</span>
<span class="c1">// adding a new column and dropping an existing column</span>
<span class="k">val</span> <span class="n">cubesDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="mi">6</span> <span class="n">to</span> <span class="mi">10</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;value&quot;</span><span class="o">,</span> <span class="s">&quot;cube&quot;</span><span class="o">)</span>
<span class="n">cubesDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table/key=2&quot;</span><span class="o">)</span>

<span class="c1">// Read the partitioned table</span>
<span class="k">val</span> <span class="n">mergedDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;mergeSchema&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">).</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table&quot;</span><span class="o">)</span>
<span class="n">mergedDF</span><span class="o">.</span><span class="n">printSchema</span><span class="o">()</span>

<span class="c1">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1">// with the partitioning column appeared in the partition directory paths</span>
<span class="c1">// root</span>
<span class="c1">// |-- value: int (nullable = true)</span>
<span class="c1">// |-- square: int (nullable = true)</span>
<span class="c1">// |-- cube: int (nullable = true)</span>
<span class="c1">// |-- key : int (nullable = true)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Square</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">value</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">square</span><span class="o">;</span>

  <span class="c1">// Getters and setters...</span>

<span class="o">}</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Cube</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">value</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">cube</span><span class="o">;</span>

  <span class="c1">// Getters and setters...</span>

<span class="o">}</span>

<span class="n">List</span><span class="o">&lt;</span><span class="n">Square</span><span class="o">&gt;</span> <span class="n">squares</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span> <span class="n">value</span> <span class="o">&lt;=</span> <span class="mi">5</span><span class="o">;</span> <span class="n">value</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">Square</span> <span class="n">square</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Square</span><span class="o">();</span>
  <span class="n">square</span><span class="o">.</span><span class="na">setValue</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
  <span class="n">square</span><span class="o">.</span><span class="na">setSquare</span><span class="o">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">value</span><span class="o">);</span>
  <span class="n">squares</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">square</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// Create a simple DataFrame, store into a partition directory</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">squaresDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">squares</span><span class="o">,</span> <span class="n">Square</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">squaresDF</span><span class="o">.</span><span class="na">write</span><span class="o">().</span><span class="na">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table/key=1&quot;</span><span class="o">);</span>

<span class="n">List</span><span class="o">&lt;</span><span class="n">Cube</span><span class="o">&gt;</span> <span class="n">cubes</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">6</span><span class="o">;</span> <span class="n">value</span> <span class="o">&lt;=</span> <span class="mi">10</span><span class="o">;</span> <span class="n">value</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">Cube</span> <span class="n">cube</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Cube</span><span class="o">();</span>
  <span class="n">cube</span><span class="o">.</span><span class="na">setValue</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
  <span class="n">cube</span><span class="o">.</span><span class="na">setCube</span><span class="o">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">value</span> <span class="o">*</span> <span class="n">value</span><span class="o">);</span>
  <span class="n">cubes</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">cube</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// Create another DataFrame in a new partition directory,</span>
<span class="c1">// adding a new column and dropping an existing column</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">cubesDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">cubes</span><span class="o">,</span> <span class="n">Cube</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">cubesDF</span><span class="o">.</span><span class="na">write</span><span class="o">().</span><span class="na">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table/key=2&quot;</span><span class="o">);</span>

<span class="c1">// Read the partitioned table</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">mergedDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;mergeSchema&quot;</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="na">parquet</span><span class="o">(</span><span class="s">&quot;data/test_table&quot;</span><span class="o">);</span>
<span class="n">mergedDF</span><span class="o">.</span><span class="na">printSchema</span><span class="o">();</span>

<span class="c1">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1">// with the partitioning column appeared in the partition directory paths</span>
<span class="c1">// root</span>
<span class="c1">//  |-- value: int (nullable = true)</span>
<span class="c1">//  |-- square: int (nullable = true)</span>
<span class="c1">//  |-- cube: int (nullable = true)</span>
<span class="c1">//  |-- key: int (nullable = true)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="c1"># spark is from the previous example.</span>
<span class="c1"># Create a simple DataFrame, stored into a partition directory</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="n">squaresDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
                                  <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="n">i</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">squaresDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=1&quot;</span><span class="p">)</span>

<span class="c1"># Create another DataFrame in a new partition directory,</span>
<span class="c1"># adding a new column and dropping an existing column</span>
<span class="n">cubesDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
                                <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">triple</span><span class="o">=</span><span class="n">i</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">cubesDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=2&quot;</span><span class="p">)</span>

<span class="c1"># Read the partitioned table</span>
<span class="n">mergedDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;mergeSchema&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table&quot;</span><span class="p">)</span>
<span class="n">mergedDF</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>

<span class="c1"># The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1"># with the partitioning column appeared in the partition directory paths.</span>
<span class="c1"># root</span>
<span class="c1">#  |-- double: long (nullable = true)</span>
<span class="c1">#  |-- single: long (nullable = true)</span>
<span class="c1">#  |-- triple: long (nullable = true)</span>
<span class="c1">#  |-- key: integer (nullable = true)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">df1</span> <span class="o">&lt;-</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">12</span><span class="p">,</span> <span class="m">29</span><span class="p">),</span> <span class="n">double</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">19</span><span class="p">,</span> <span class="m">23</span><span class="p">)))</span>
<span class="n">df2</span> <span class="o">&lt;-</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">double</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">19</span><span class="p">,</span> <span class="m">23</span><span class="p">),</span> <span class="n">triple</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">23</span><span class="p">,</span> <span class="m">18</span><span class="p">)))</span>

<span class="c1"># Create a simple DataFrame, stored into a partition directory</span>
<span class="nf">write.df</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="s">&quot;data/test_table/key=1&quot;</span><span class="p">,</span> <span class="s">&quot;parquet&quot;</span><span class="p">,</span> <span class="s">&quot;overwrite&quot;</span><span class="p">)</span>

<span class="c1"># Create another DataFrame in a new partition directory,</span>
<span class="c1"># adding a new column and dropping an existing column</span>
<span class="nf">write.df</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s">&quot;data/test_table/key=2&quot;</span><span class="p">,</span> <span class="s">&quot;parquet&quot;</span><span class="p">,</span> <span class="s">&quot;overwrite&quot;</span><span class="p">)</span>

<span class="c1"># Read the partitioned table</span>
<span class="n">df3</span> <span class="o">&lt;-</span> <span class="nf">read.df</span><span class="p">(</span><span class="s">&quot;data/test_table&quot;</span><span class="p">,</span> <span class="s">&quot;parquet&quot;</span><span class="p">,</span> <span class="n">mergeSchema</span> <span class="o">=</span> <span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="nf">printSchema</span><span class="p">(</span><span class="n">df3</span><span class="p">)</span>
<span class="c1"># The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1"># with the partitioning column appeared in the partition directory paths</span>
<span class="c1">## root</span>
<span class="c1">##  |-- single: double (nullable = true)</span>
<span class="c1">##  |-- double: double (nullable = true)</span>
<span class="c1">##  |-- triple: double (nullable = true)</span>
<span class="c1">##  |-- key: integer (nullable = true)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
</div>
<div class="section" id="hive-metastore-parquet">
<h4>Hive metastore Parquet表转换<a class="headerlink" href="#hive-metastore-parquet" title="永久链接至标题">¶</a></h4>
<p>当读写Hive metastore Parquet表时，为了达到更好的性能, Spark SQL使用它自己的Parquet支持库，而不是Hive SerDe。这一行为是由 spark.sql.hive.convertMetastoreParquet 这个配置项来控制的，它默认是开启的。</p>
<div class="section" id="hive-parquet-schema">
<h5>Hive/Parquet Schema调整<a class="headerlink" href="#hive-parquet-schema" title="永久链接至标题">¶</a></h5>
<p>从表 schema 处理的角度来看, Hive和Parquet有2个关键的不同点：</p>
<ol class="arabic simple">
<li><p>Hive是非大小写敏感的，而Parquet是大小写敏感的。</p></li>
<li><p>Hive认为所有列都是nullable，而Parquet中为空性是很重要的。</p></li>
</ol>
<p>基于以上原因，在将一个Hive metastore Parquet表转换成一个Spark SQL Parquet表的时候，必须要对Hive metastore schema做调整，调整规则如下：</p>
<ol class="arabic simple">
<li><p>两个schema中字段名称一致的话那么字段类型也必须一致（不考虑为空性）。调整后的字段应该有Parquet端的数据类型，所以为空性也是需要考虑的。</p></li>
<li><dl class="simple">
<dt>调整后的schema必须完全包含Hive metastore schema中定义的字段。</dt><dd><ul class="simple">
<li><p>只出现在Parquet schema中的字段将在调整后的schema中丢弃。</p></li>
<li><p>只出现在Hive metastore schema中的字段将作为nullable字段添加到调整后的schema。</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id10">
<h5>元数据刷新<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h5>
<p>Spark SQL 会缓存 Parquet 元数据以提高性能。如果启用了Hive metastore Parquet table转换，那么转换后的表的schema也会被缓存起来。如果这些表被Hive或其它外部工具更新, 那么你需要手动地刷新它们以确保元数据一致性。</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">refreshTable</span><span class="o">(</span><span class="s">&quot;my_table&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="c1">// spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="na">catalog</span><span class="o">().</span><span class="na">refreshTable</span><span class="o">(</span><span class="s">&quot;my_table&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing HiveContext</span>
<span class="n">spark</span><span class="o">.</span><span class="n">refreshTable</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="n">REFRESH</span> <span class="k">TABLE</span> <span class="n">my_table</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id11">
<h4>配置<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h4>
<p>Parquet配置可以使用 SparkSession 上的 setConf 方法或者使用 SQL 语句中的 SET key=value 命令来完成。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 4%" />
<col style="width: 79%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spark.sql.parquet.binaryAsString</p></td>
<td><p>false</p></td>
<td><p>一些其它的Parquet生产系统, 特别是Impala，Hive以及老版本的Spark SQL，当写Parquet schema时都不区分二进制数据和字符串。这个标识告诉Spark SQL把二进制数据当字符串处理，以兼容老系统。</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.parquet.int96AsTimestamp</p></td>
<td><p>true</p></td>
<td><p>一些Parquet生产系统, 特别是Impala和Hive，把时间戳存成INT96。这个标识告诉Spark SQL将INT96数据解析成timestamp，以兼容老系统。</p></td>
</tr>
<tr class="row-even"><td><p>spark.sql.parquet.cacheMetadata</p></td>
<td><p>true</p></td>
<td><p>开启Parquet schema元数据缓存。可以提升查询静态数据的速度。</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.parquet.compression.codec</p></td>
<td><p>gzip</p></td>
<td><p>当写Parquet文件时，设置压缩编码格式。可接受的值有：uncompressed, snappy, gzip, lzo</p></td>
</tr>
<tr class="row-even"><td><p>spark.sql.parquet.filterPushdown</p></td>
<td><p>true</p></td>
<td><p>当设置为true时启用Parquet过滤器下推优化</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.hive.convertMetastoreParquet</p></td>
<td><p>true</p></td>
<td><p>当设置为false时，Spark SQL将使用Hive SerDe，而不是内建的Parquet tables支持</p></td>
</tr>
<tr class="row-even"><td><p>spark.sql.parquet.mergeSchema</p></td>
<td><p>false</p></td>
<td><p>如果设为true，那么Parquet数据源将会合并所有数据文件的schema，否则，从汇总文件中选取schema，如果没有汇总文件，则随机选取一个数据文件）</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="json-datasets">
<h3>JSON Datasets<a class="headerlink" href="#json-datasets" title="永久链接至标题">¶</a></h3>
<p><strong>Scala</strong></p>
<p>Spark SQL可以自动推导JSON数据集的schema并且将其加载为一个 Dataset[Row]。这种转换可以在一个包含String的RDD或一个JSON文件上使用SparkSession.read.json() 来完成。</p>
<p>注意，作为json文件提供的文件并不是一个典型的JSON文件。JSON文件的每一行必须包含一个独立的、完整有效的JSON对象。因此，一个常规的多行json文件经常会加载失败。</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span>
<span class="c1">// supported by importing this when creating a Dataset.</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="c1">// A JSON dataset is pointed to by path.</span>
<span class="c1">// The path can be either a single text file or a directory storing text files</span>
<span class="k">val</span> <span class="n">path</span> <span class="k">=</span> <span class="s">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="k">val</span> <span class="n">peopleDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="n">path</span><span class="o">)</span>

<span class="c1">// The inferred schema can be visualized using the printSchema() method</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">//  |-- age: long (nullable = true)</span>
<span class="c1">//  |-- name: string (nullable = true)</span>

<span class="c1">// Creates a temporary view using the DataFrame</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>

<span class="c1">// SQL statements can be run by using the sql methods provided by spark</span>
<span class="k">val</span> <span class="n">teenagerNamesDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">)</span>
<span class="n">teenagerNamesDF</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +------+</span>
<span class="c1">// |  name|</span>
<span class="c1">// +------+</span>
<span class="c1">// |Justin|</span>
<span class="c1">// +------+</span>

<span class="c1">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="c1">// a Dataset[String] storing one JSON object per string</span>
<span class="k">val</span> <span class="n">otherPeopleDataset</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataset</span><span class="o">(</span>
  <span class="s">&quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot;</span> <span class="o">::</span> <span class="nc">Nil</span><span class="o">)</span>
<span class="k">val</span> <span class="n">otherPeople</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="n">otherPeopleDataset</span><span class="o">)</span>
<span class="n">otherPeople</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---------------+----+</span>
<span class="c1">// |        address|name|</span>
<span class="c1">// +---------------+----+</span>
<span class="c1">// |[Columbus,Ohio]| Yin|</span>
<span class="c1">// +---------------+----+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>Spark SQL 可以自动推导 JSON 数据集的 schema 并且将其加载为一个 Dataset&lt;Row&gt;. 这种转换可以在一个包含String的RDD或一个JSON文件上使用SparkSession.read.json() 来完成。</p>
<p>注意，作为json文件提供的文件并不是一个典型的JSON文件。JSON文件的每一行必须包含一个独立的、完整有效的JSON对象。因此，一个常规的多行json文件经常会加载失败。</p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>

<span class="c1">// A JSON dataset is pointed to by path.</span>
<span class="c1">// The path can be either a single text file or a directory storing text files</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">people</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">);</span>

<span class="c1">// The inferred schema can be visualized using the printSchema() method</span>
<span class="n">people</span><span class="o">.</span><span class="na">printSchema</span><span class="o">();</span>
<span class="c1">// root</span>
<span class="c1">//  |-- age: long (nullable = true)</span>
<span class="c1">//  |-- name: string (nullable = true)</span>

<span class="c1">// Creates a temporary view using the DataFrame</span>
<span class="n">people</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">);</span>

<span class="c1">// SQL statements can be run by using the sql methods provided by spark</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">namesDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span><span class="o">);</span>
<span class="n">namesDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +------+</span>
<span class="c1">// |  name|</span>
<span class="c1">// +------+</span>
<span class="c1">// |Justin|</span>
<span class="c1">// +------+</span>

<span class="c1">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="c1">// a Dataset&lt;String&gt; storing one JSON object per string.</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">jsonData</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span>
        <span class="s">&quot;{\&quot;name\&quot;:\&quot;Yin\&quot;,\&quot;address\&quot;:{\&quot;city\&quot;:\&quot;Columbus\&quot;,\&quot;state\&quot;:\&quot;Ohio\&quot;}}&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">anotherPeopleDataset</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataset</span><span class="o">(</span><span class="n">jsonData</span><span class="o">,</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">anotherPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">json</span><span class="o">(</span><span class="n">anotherPeopleDataset</span><span class="o">);</span>
<span class="n">anotherPeople</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---------------+----+</span>
<span class="c1">// |        address|name|</span>
<span class="c1">// +---------------+----+</span>
<span class="c1">// |[Columbus,Ohio]| Yin|</span>
<span class="c1">// +---------------+----+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<p>Spark SQL可以自动推导JSON数据集的schema并且将其加载为一个 DataFrame。这种转换可以在一个JSON文件上使用SparkSession.read.json 来完成。</p>
<p>注意，作为 json 文件提供的文件并不是一个典型的 JSON 文件。JSON 文件的每一行必须包含一个独立的、完整有效的JSON对象。因此，一个常规的多行json文件经常会加载失败。</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># spark is from the previous example.</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># A JSON dataset is pointed to by path.</span>
<span class="c1"># The path can be either a single text file or a directory storing text files</span>
<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="n">peopleDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># The inferred schema can be visualized using the printSchema() method</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1"># root</span>
<span class="c1">#  |-- age: long (nullable = true)</span>
<span class="c1">#  |-- name: string (nullable = true)</span>

<span class="c1"># Creates a temporary view using the DataFrame</span>
<span class="n">peopleDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL statements can be run by using the sql methods provided by spark</span>
<span class="n">teenagerNamesDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span><span class="p">)</span>
<span class="n">teenagerNamesDF</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +------+</span>
<span class="c1"># |  name|</span>
<span class="c1"># +------+</span>
<span class="c1"># |Justin|</span>
<span class="c1"># +------+</span>

<span class="c1"># Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="c1"># an RDD[String] storing one JSON object per string</span>
<span class="n">jsonStrings</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&#39;</span><span class="p">]</span>
<span class="n">otherPeopleRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">jsonStrings</span><span class="p">)</span>
<span class="n">otherPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">otherPeopleRDD</span><span class="p">)</span>
<span class="n">otherPeople</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---------------+----+</span>
<span class="c1"># |        address|name|</span>
<span class="c1"># +---------------+----+</span>
<span class="c1"># |[Columbus,Ohio]| Yin|</span>
<span class="c1"># +---------------+----+</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. using the read.json() function, which loads data from a directory of JSON files where each line of the files is a JSON object.</p>
<p>Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. For more information, please see JSON Lines text format, also called newline-delimited JSON.</p>
<p>For a regular multi-line JSON file, set a named parameter multiLine to TRUE.</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="c1"># A JSON dataset is pointed to by path.</span>
<span class="c1"># The path can be either a single text file or a directory storing text files.</span>
<span class="n">path</span> <span class="o">&lt;-</span> <span class="s">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="c1"># Create a DataFrame from the file(s) pointed to by path</span>
<span class="n">people</span> <span class="o">&lt;-</span> <span class="nf">read.json</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># The inferred schema can be visualized using the printSchema() method.</span>
<span class="nf">printSchema</span><span class="p">(</span><span class="n">people</span><span class="p">)</span>
<span class="c1">## root</span>
<span class="c1">##  |-- age: long (nullable = true)</span>
<span class="c1">##  |-- name: string (nullable = true)</span>

<span class="c1"># Register this DataFrame as a table.</span>
<span class="nf">createOrReplaceTempView</span><span class="p">(</span><span class="n">people</span><span class="p">,</span> <span class="s">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL statements can be run by using the sql methods.</span>
<span class="n">teenagers</span> <span class="o">&lt;-</span> <span class="nf">sql</span><span class="p">(</span><span class="s">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">teenagers</span><span class="p">)</span>
<span class="c1">##     name</span>
<span class="c1">## 1 Justin</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TEMPORARY</span> <span class="k">VIEW</span> <span class="n">jsonTable</span>
<span class="k">USING</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="k">sql</span><span class="p">.</span><span class="n">json</span>
<span class="k">OPTIONS</span> <span class="p">(</span>
  <span class="n">path</span> <span class="ss">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="p">)</span>

<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">jsonTable</span>
</pre></div>
</div>
</div>
<div class="section" id="hive-tables">
<h3>Hive Tables<a class="headerlink" href="#hive-tables" title="永久链接至标题">¶</a></h3>
<p>Spark SQL 还支持从 Apache Hive 读写数据。然而，由于 Hive 依赖项太多，这些依赖没有包含在默认的 Spark 发行版本中。如果在classpath上配置了Hive依赖，那么 Spark 会自动加载它们。注意，Hive 依赖也必须放到所有的 worker 节点上，因为如果要访问 Hive 中的数据它们需要访问 Hive 序列化和反序列化库(SerDes)。</p>
<p>Hive配置是通过将 hive-site.xml，core-site.xml(用于安全配置)以及 hdfs-site.xml(用于 HDFS 配置)文件放置在 conf/ 目录下来完成的。</p>
<p>如果要使用 Hive, 你必须要实例化一个支持 Hive 的 SparkSession, 包括连接到一个持久化的 Hive metastore, 支持 Hive serdes 以及 Hive 用户自定义函数。即使用户没有安装部署 Hive 也仍然可以启用Hive支持。如果没有在 hive-site.xml 文件中配置, Spark 应用程序启动之后，上下文会自动在当前目录下创建一个 metastore_db 目录并创建一个由 spark.sql.warehouse.dir 配置的、默认值是当前目录下的 spark-warehouse 目录的目录。请注意: 从 Spark 2.0.0 版本开始, hive-site.xml 中的 hive.metastore.warehouse.dir 属性就已经过时了, 你可以使用 spark.sql.warehouse.dir 来指定仓库中数据库的默认存储位置。你可能还需要给启动Spark应用程序的用户赋予写权限。</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">java.io.File</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.Row</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Record</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="c1">// warehouseLocation points to the default location for managed databases and tables</span>
<span class="k">val</span> <span class="n">warehouseLocation</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">File</span><span class="o">(</span><span class="s">&quot;spark-warehouse&quot;</span><span class="o">).</span><span class="n">getAbsolutePath</span>

<span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&quot;Spark Hive Example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="s">&quot;spark.sql.warehouse.dir&quot;</span><span class="o">,</span> <span class="n">warehouseLocation</span><span class="o">)</span>
  <span class="o">.</span><span class="n">enableHiveSupport</span><span class="o">()</span>
  <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">spark.sql</span>

<span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="o">)</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="o">)</span>

<span class="c1">// Queries are expressed in HiveQL</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Aggregation queries are also supported.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------+</span>
<span class="c1">// |count(1)|</span>
<span class="c1">// +--------+</span>
<span class="c1">// |    500 |</span>
<span class="c1">// +--------+</span>

<span class="c1">// The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="k">val</span> <span class="n">sqlDF</span> <span class="k">=</span> <span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="o">)</span>

<span class="c1">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span>
<span class="k">val</span> <span class="n">stringsDS</span> <span class="k">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Row</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="s">s&quot;Key: </span><span class="si">$key</span><span class="s">, Value: </span><span class="si">$value</span><span class="s">&quot;</span>
<span class="o">}</span>
<span class="n">stringsDS</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |               value|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// ...</span>

<span class="c1">// You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="k">val</span> <span class="n">recordsDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">((</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">100</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="nc">Record</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="s">s&quot;val_</span><span class="si">$i</span><span class="s">&quot;</span><span class="o">)))</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;records&quot;</span><span class="o">)</span>

<span class="c1">// Queries can then join DataFrame data with data stored in Hive.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |key| value|key| value|</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  4| val_4|  4| val_4|</span>
<span class="c1">// |  5| val_5|  5| val_5|</span>
<span class="c1">// ...</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.io.File</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Record</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">key</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">value</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getKey</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">key</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setKey</span><span class="o">(</span><span class="kt">int</span> <span class="n">key</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="n">String</span> <span class="nf">getValue</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">value</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setValue</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// warehouseLocation points to the default location for managed databases and tables</span>
<span class="n">String</span> <span class="n">warehouseLocation</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="s">&quot;spark-warehouse&quot;</span><span class="o">).</span><span class="na">getAbsolutePath</span><span class="o">();</span>
<span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
  <span class="o">.</span><span class="na">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="na">appName</span><span class="o">(</span><span class="s">&quot;Java Spark Hive Example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">config</span><span class="o">(</span><span class="s">&quot;spark.sql.warehouse.dir&quot;</span><span class="o">,</span> <span class="n">warehouseLocation</span><span class="o">)</span>
  <span class="o">.</span><span class="na">enableHiveSupport</span><span class="o">()</span>
  <span class="o">.</span><span class="na">getOrCreate</span><span class="o">();</span>

<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="o">);</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="o">);</span>

<span class="c1">// Queries are expressed in HiveQL</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM src&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Aggregation queries are also supported.</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------+</span>
<span class="c1">// |count(1)|</span>
<span class="c1">// +--------+</span>
<span class="c1">// |    500 |</span>
<span class="c1">// +--------+</span>

<span class="c1">// The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="o">);</span>

<span class="c1">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">stringsDS</span> <span class="o">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">row</span> <span class="o">-&gt;</span> <span class="s">&quot;Key: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot;, Value: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span>
    <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
<span class="n">stringsDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |               value|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// ...</span>

<span class="c1">// You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">Record</span><span class="o">&gt;</span> <span class="n">records</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">key</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span> <span class="n">key</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="o">;</span> <span class="n">key</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">Record</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Record</span><span class="o">();</span>
  <span class="n">record</span><span class="o">.</span><span class="na">setKey</span><span class="o">(</span><span class="n">key</span><span class="o">);</span>
  <span class="n">record</span><span class="o">.</span><span class="na">setValue</span><span class="o">(</span><span class="s">&quot;val_&quot;</span> <span class="o">+</span> <span class="n">key</span><span class="o">);</span>
  <span class="n">records</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
<span class="o">}</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">recordsDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">records</span><span class="o">,</span> <span class="n">Record</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;records&quot;</span><span class="o">);</span>

<span class="c1">// Queries can then join DataFrames data with data stored in Hive.</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |key| value|key| value|</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  4| val_4|  4| val_4|</span>
<span class="c1">// ...</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">expanduser</span><span class="p">,</span> <span class="n">join</span><span class="p">,</span> <span class="n">abspath</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="c1"># warehouse_location points to the default location for managed databases and tables</span>
<span class="n">warehouse_location</span> <span class="o">=</span> <span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;spark-warehouse&#39;</span><span class="p">)</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL Hive integration example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.warehouse.dir&quot;</span><span class="p">,</span> <span class="n">warehouse_location</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries are expressed in HiveQL</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |key|  value|</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |238|val_238|</span>
<span class="c1"># | 86| val_86|</span>
<span class="c1"># |311|val_311|</span>
<span class="c1"># ...</span>

<span class="c1"># Aggregation queries are also supported.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +--------+</span>
<span class="c1"># |count(1)|</span>
<span class="c1"># +--------+</span>
<span class="c1"># |    500 |</span>
<span class="c1"># +--------+</span>

<span class="c1"># The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="p">)</span>

<span class="c1"># The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span>
<span class="n">stringsDS</span> <span class="o">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="s2">&quot;Key: </span><span class="si">%d</span><span class="s2">, Value: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
<span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">stringsDS</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># ...</span>

<span class="c1"># You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="n">Record</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>
<span class="n">recordsDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Record</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s2">&quot;val_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)])</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>

<span class="c1"># Queries can then join DataFrame data with data stored in Hive.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |key| value|key| value|</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |  2| val_2|  2| val_2|</span>
<span class="c1"># |  4| val_4|  4| val_4|</span>
<span class="c1"># |  5| val_5|  5| val_5|</span>
<span class="c1"># ...</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/hive.py” 文件。</p>
<p><strong>R</strong></p>
<p>When working with Hive one must instantiate SparkSession with Hive support. This adds support for finding tables in the MetaStore and writing queries using HiveQL.</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="c1"># enableHiveSupport defaults to TRUE</span>
<span class="nf">sparkR.session</span><span class="p">(</span><span class="n">enableHiveSupport</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
<span class="nf">sql</span><span class="p">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="p">)</span>
<span class="nf">sql</span><span class="p">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries can be expressed in HiveQL.</span>
<span class="n">results</span> <span class="o">&lt;-</span> <span class="nf">collect</span><span class="p">(</span><span class="nf">sql</span><span class="p">(</span><span class="s">&quot;FROM src SELECT key, value&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件</p>
</div>
<div class="section" id="specifying-storage-format-for-hive-tables">
<h3>Specifying storage format for Hive tables<a class="headerlink" href="#specifying-storage-format-for-hive-tables" title="永久链接至标题">¶</a></h3>
<p>When you create a Hive table, you need to define how this table should read/write data from/to file system, i.e. the “input format” and “output format”. You also need to define how this table should deserialize the data to rows, or serialize rows to data, i.e. the “serde”. The following options can be used to specify the storage format(“serde”, “input format”, “output format”), e.g. CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’). By default, we will read the table files as plain text. Note that, Hive storage handler is not supported yet when creating table, you can create a table using storage handler at Hive side, and use Spark SQL to read it.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fileFormat</p></td>
<td><p>A fileFormat is kind of a package of storage format specifications, including “serde”, “input format” and “output format”. Currently we support 6 fileFormats: ‘sequencefile’, ‘rcfile’, ‘orc’, ‘parquet’, ‘textfile’ and ‘avro’.</p></td>
</tr>
<tr class="row-odd"><td><p>inputFormat, outputFormat</p></td>
<td><p>These 2 options specify the name of a corresponding <cite>InputFormat</cite> and <cite>OutputFormat</cite> class as a string literal, e.g. <cite>org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</cite>. These 2 options must be appeared in pair, and you can not specify them if you already specified the <cite>fileFormat</cite> option.</p></td>
</tr>
<tr class="row-even"><td><p>serde</p></td>
<td><p>This option specifies the name of a serde class. When the <cite>fileFormat</cite> option is specified, do not specify this option if the given <cite>fileFormat</cite> already include the information of serde. Currently “sequencefile”, “textfile” and “rcfile” don’t include the serde information and you can use this option with these 3 fileFormats.</p></td>
</tr>
<tr class="row-odd"><td><p>fieldDelim, escapeDelim,</p></td>
<td><p>These options can only be used with “textfile” fileFormat. They define how to read delimited files into rows.</p></td>
</tr>
<tr class="row-even"><td><p>collectionDelim, mapkeyDelim, lineDelim</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All other properties defined with OPTIONS will be regarded as Hive serde properties.</p>
</div>
<div class="section" id="hive-metastore">
<h3>与不同版本的Hive Metastore交互<a class="headerlink" href="#hive-metastore" title="永久链接至标题">¶</a></h3>
<p>Spark SQL对Hive最重要的一个支持就是可以和Hive metastore进行交互，这使得Spark SQL可以访问Hive表的元数据。从Spark 1.4.0版本开始，通过使用下面描述的配置, Spark SQL一个简单的二进制编译版本可以用来查询不同版本的Hive metastore。注意，不管用于访问 metastore的Hive是什么版本，Spark SQL内部都使用 Hive 1.2.1 版本进行编译, 并且使用这个版本的一些类用于内部执行（serdes，UDFs，UDAFs等）。</p>
<p>下面的选项可用来配置用于检索元数据的Hive版本：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 6%" />
<col style="width: 86%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spark.sql.hive.metastore.version</p></td>
<td><p>1.2.1</p></td>
<td><p>Version of the Hive metastore. Available options are 0.12.0 through 1.2.1.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.hive.metastore.jars</p></td>
<td><p>builtin</p></td>
<td><blockquote>
<div><p>Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options:</p>
</div></blockquote>
<p>builtin  Use Hive 1.2.1, which is bundled with the Spark assembly when -Phive is enabled. When this option is chosen, spark.sql.hive.metastore.version must be either 1.2.1 or not defined.
maven Use Hive jars of specified version downloaded from Maven repositories. This configuration is not generally recommended for production deployments.
A classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with your application.</p>
</td>
</tr>
<tr class="row-even"><td><p>spark.sql.hive.metastore.sharedPrefixes</p></td>
<td><p>com.mysql.jdbc,
org.postgresql,
com.microsoft.sqlserver,
oracle.jdbc</p></td>
<td><p>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.hive.metastore.barrierPrefixes</p></td>
<td><p>(empty)</p></td>
<td><p>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. org.apache.spark.*).</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="jdbc-to-other-databases">
<h3>JDBC To Other Databases<a class="headerlink" href="#jdbc-to-other-databases" title="永久链接至标题">¶</a></h3>
<p>Spark SQL也包括一个可以使用JDBC从其它数据库读取数据的数据源。该功能应该优于使用JdbcRDD，因为它的返回结果是一个DataFrame，而在Spark SQL中DataFrame处理简单，且和其它数据源进行关联操作。JDBC数据源在Java和Python中用起来很简单，因为不需要用户提供一个ClassTag。（注意，这和 Spark SQL JDBC server不同，Spark SQL JDBC server 允许其它应用程序使用Spark SQL执行查询）</p>
<p>首先，你需要在 Spark classpath 中包含对应数据库的 JDBC driver。例如，为了从 Spark Shell 连接到 postgres 数据库，你需要运行下面的命令：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
</pre></div>
</div>
<p>通过使用 Data Sources API, 远程数据库的表可以加载为一个 DataFrame 或 Spark SQL 临时表。支持的选项如下：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 8%" />
<col style="width: 92%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>url</p></td>
<td><p>The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <a class="reference external" href="jdbc:postgresql://localhost/test?user=fred&amp;password=secret">jdbc:postgresql://localhost/test?user=fred&amp;password=secret</a></p></td>
</tr>
<tr class="row-odd"><td><p>dbtable</p></td>
<td><p>The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.</p></td>
</tr>
<tr class="row-even"><td><p>driver</p></td>
<td><p>The class name of the JDBC driver to use to connect to this URL.</p></td>
</tr>
<tr class="row-odd"><td><p>partitionColumn, lowerBound, upperBound</p></td>
<td><p>These options must all be specified if any of them is specified. In addition, numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</p></td>
</tr>
<tr class="row-even"><td><p>numPartitions</p></td>
<td><p>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing.</p></td>
</tr>
<tr class="row-odd"><td><p>fetchsize</p></td>
<td><p>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</p></td>
</tr>
<tr class="row-even"><td><p>batchsize</p></td>
<td><p>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to 1000.</p></td>
</tr>
<tr class="row-odd"><td><p>isolationLevel</p></td>
<td><p>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of READ_UNCOMMITTED. This option applies only to writing. Please refer the documentation in java.sql.Connection.</p></td>
</tr>
<tr class="row-even"><td><p>truncate</p></td>
<td><p>This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to false. This option applies only to writing.</p></td>
</tr>
<tr class="row-odd"><td><p>createTableOptions</p></td>
<td><p>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). This option applies only to writing.</p></td>
</tr>
<tr class="row-even"><td><p>createTableColumnTypes</p></td>
<td><p>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: “name CHAR(64), comments VARCHAR(1024)”). The specified types should be valid spark sql data types. This option applies only to writing.</p></td>
</tr>
</tbody>
</table>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span>
<span class="c1">// Loading data from a JDBC source</span>
<span class="k">val</span> <span class="n">jdbcDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;jdbc&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;url&quot;</span><span class="o">,</span> <span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;dbtable&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">load</span><span class="o">()</span>

<span class="k">val</span> <span class="n">connectionProperties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="n">connectionProperties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">)</span>
<span class="n">connectionProperties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">jdbcDF2</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span>
  <span class="o">.</span><span class="n">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">)</span>

<span class="c1">// Saving data to a JDBC source</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;jdbc&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;url&quot;</span><span class="o">,</span> <span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;dbtable&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">save</span><span class="o">()</span>

<span class="n">jdbcDF2</span><span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">)</span>

<span class="c1">// Specifying create table column data types on write</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;createTableColumnTypes&quot;</span><span class="o">,</span> <span class="s">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span>
<span class="c1">// Loading data from a JDBC source</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">jdbcDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;jdbc&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;url&quot;</span><span class="o">,</span> <span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;dbtable&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">load</span><span class="o">();</span>

<span class="n">Properties</span> <span class="n">connectionProperties</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Properties</span><span class="o">();</span>
<span class="n">connectionProperties</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">);</span>
<span class="n">connectionProperties</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">jdbcDF2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">);</span>

<span class="c1">// Saving data to a JDBC source</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="na">write</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;jdbc&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;url&quot;</span><span class="o">,</span> <span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;dbtable&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;user&quot;</span><span class="o">,</span> <span class="s">&quot;username&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;password&quot;</span><span class="o">,</span> <span class="s">&quot;password&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">save</span><span class="o">();</span>

<span class="n">jdbcDF2</span><span class="o">.</span><span class="na">write</span><span class="o">()</span>
  <span class="o">.</span><span class="na">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">);</span>

<span class="c1">// Specifying create table column data types on write</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="na">write</span><span class="o">()</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;createTableColumnTypes&quot;</span><span class="o">,</span> <span class="s">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">jdbc</span><span class="o">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="o">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="o">,</span> <span class="n">connectionProperties</span><span class="o">);</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java” 文件。</p>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span>
<span class="c1"># Loading data from a JDBC source</span>
<span class="n">jdbcDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;jdbc&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="s2">&quot;schema.tablename&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="n">jdbcDF2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
    <span class="o">.</span><span class="n">jdbc</span><span class="p">(</span><span class="s2">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span> <span class="s2">&quot;schema.tablename&quot;</span><span class="p">,</span>
          <span class="n">properties</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="s2">&quot;username&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;password&quot;</span><span class="p">})</span>

<span class="c1"># Saving data to a JDBC source</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="n">write</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;jdbc&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="s2">&quot;schema.tablename&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="n">jdbcDF2</span><span class="o">.</span><span class="n">write</span> \
    <span class="o">.</span><span class="n">jdbc</span><span class="p">(</span><span class="s2">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span> <span class="s2">&quot;schema.tablename&quot;</span><span class="p">,</span>
          <span class="n">properties</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="s2">&quot;username&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;password&quot;</span><span class="p">})</span>

<span class="c1"># Specifying create table column data types on write</span>
<span class="n">jdbcDF</span><span class="o">.</span><span class="n">write</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;createTableColumnTypes&quot;</span><span class="p">,</span> <span class="s2">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">jdbc</span><span class="p">(</span><span class="s2">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span> <span class="s2">&quot;schema.tablename&quot;</span><span class="p">,</span>
          <span class="n">properties</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="s2">&quot;username&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;password&quot;</span><span class="p">})</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/python/sql/datasource.py” 文件。</p>
<p><strong>R</strong></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading data from a JDBC source</span>
<span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">read.jdbc</span><span class="p">(</span><span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="p">,</span> <span class="n">user</span> <span class="o">=</span> <span class="s">&quot;username&quot;</span><span class="p">,</span> <span class="n">password</span> <span class="o">=</span> <span class="s">&quot;password&quot;</span><span class="p">)</span>

<span class="c1"># Saving data to a JDBC source</span>
<span class="nf">write.jdbc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span> <span class="s">&quot;schema.tablename&quot;</span><span class="p">,</span> <span class="n">user</span> <span class="o">=</span> <span class="s">&quot;username&quot;</span><span class="p">,</span> <span class="n">password</span> <span class="o">=</span> <span class="s">&quot;password&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/r/RSparkSQLExample.R” 文件。</p>
<p><strong>Sql</strong></p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TEMPORARY</span> <span class="k">VIEW</span> <span class="n">jdbcTable</span>
<span class="k">USING</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="k">sql</span><span class="p">.</span><span class="n">jdbc</span>
<span class="k">OPTIONS</span> <span class="p">(</span>
  <span class="n">url</span> <span class="ss">&quot;jdbc:postgresql:dbserver&quot;</span><span class="p">,</span>
  <span class="n">dbtable</span> <span class="ss">&quot;schema.tablename&quot;</span><span class="p">,</span>
  <span class="k">user</span> <span class="s1">&#39;username&#39;</span><span class="p">,</span>
  <span class="n">password</span> <span class="s1">&#39;password&#39;</span>
<span class="p">)</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">jdbcTable</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">resultTable</span>
</pre></div>
</div>
</div>
<div class="section" id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>在client session以及所有的executor上，JDBC驱动器类必须对启动类加载器可见。这是因为Java的DriverManager类在打开一个连接之前会做一个安全检查，这样就导致它忽略了对于启动类加载器不可见的所有驱动器。一种简单的方法就是修改所有worker节点上的compute_classpath.sh以包含你驱动器的jar包。</p></li>
<li><p>有些数据库，比如H2，会把所有的名称转换成大写。在Spark SQL中你也需要使用大写来引用这些名称。</p></li>
</ul>
</div>
</div>
<div class="section" id="id12">
<h2>性能调优<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h2>
<p>对于有一定计算量的Spark任务，可以将数据放入内存缓存或开启一些试验选项来提升性能。</p>
<div class="section" id="id13">
<h3>缓存数据到内存中<a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h3>
<p>通过调用 spark.cacheTable(“tableName”) 或者 dataFrame.cache() 方法, Spark SQL可以使用一种内存列存储格式缓存表。接着Spark SQL只扫描必要的列，并且自动调整压缩比例，以最小化内存占用和GC压力。你可以调用 spark.uncacheTable(“tableName”) 方法删除内存中的表。</p>
<p>内存缓存配置可以使用 SparkSession 类中的 setConf 方法或在SQL语句中运行 SET key=value命令来完成。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 6%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spark.sql.inMemoryColumnarStorage.compressed</p></td>
<td><p>true</p></td>
<td><p>如果设置为true，Spark SQL将会基于统计数据自动地为每一列选择一种压缩编码方式。</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.inMemoryColumnarStorage.batchSize</p></td>
<td><p>10000</p></td>
<td><p>控制列式缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来OOM（Out Of Memory）的风险。</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id14">
<h3>其它配置选项<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h3>
<p>下面的选项也可以用来提升执行的查询性能。随着Spark自动地执行越来越多的优化操作, 这些选项在未来的发布版本中可能会过时。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 8%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>属性名</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spark.sql.files.maxPartitionBytes</p></td>
<td><p>134217728 (128 MB)</p></td>
<td><p>读取文件时单个分区可容纳的最大字节数</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.files.openCostInBytes</p></td>
<td><p>4194304 (4 MB)</p></td>
<td><p>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度).</p></td>
</tr>
<tr class="row-even"><td><p>spark.sql.autoBroadcastJoinThreshold</p></td>
<td><p>10485760 (10 MB)</p></td>
<td><p>用于配置一个表在执行 join 操作时能够广播给所有worker节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan 命令的Hive Metastore表。</p></td>
</tr>
<tr class="row-odd"><td><p>spark.sql.shuffle.partitions</p></td>
<td><p>200</p></td>
<td><p>用于配置join或聚合操作混洗（shuffle）数据时使用的分区数。</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="id15">
<h2>分布式 SQL 引擎<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h2>
<p>通过使用 JDBC/ODBC或者命令行接口，Spark SQL还可以作为一个分布式查询引擎。在这种模式下，终端用户或应用程序可以运行SQL查询来直接与Spark SQL交互，而不需要编写任何代码。</p>
<div class="section" id="thrift-jdbc-odbc">
<h3>运行 Thrift JDBC/ODBC 服务器<a class="headerlink" href="#thrift-jdbc-odbc" title="永久链接至标题">¶</a></h3>
<p>这里实现的Thrift JDBC/ODBC server对应于Hive 1.2.1 版本中的HiveServer2。你可以使用Spark或者Hive 1.2.1自带的beeline脚本来测试这个JDBC server。</p>
<p>要启动JDBC/ODBC server， 需要在Spark安装目录下运行下面这个命令：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./sbin/start-thriftserver.sh
</pre></div>
</div>
<p>这个脚本能接受所有 bin/spark-submit 命令行选项，外加一个用于指定Hive属性的 –hiveconf 选项。你可以运行 ./sbin/start-thriftserver.sh —help 来查看所有可用选项的完整列表。默认情况下，启动的 server 将会在 localhost:10000 上进行监听。你可以覆盖该行为, 比如使用以下环境变量：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">HIVE_SERVER2_THRIFT_PORT</span><span class="o">=</span>&lt;listening-port&gt;
<span class="nb">export</span> <span class="nv">HIVE_SERVER2_THRIFT_BIND_HOST</span><span class="o">=</span>&lt;listening-host&gt;
./sbin/start-thriftserver.sh <span class="se">\</span>
  --master &lt;master-uri&gt; <span class="se">\</span>
  ...
</pre></div>
</div>
<p>或者系统属性：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./sbin/start-thriftserver.sh <span class="se">\</span>
  --hiveconf hive.server2.thrift.port<span class="o">=</span>&lt;listening-port&gt; <span class="se">\</span>
  --hiveconf hive.server2.thrift.bind.host<span class="o">=</span>&lt;listening-host&gt; <span class="se">\</span>
  --master &lt;master-uri&gt;
  ...
</pre></div>
</div>
<p>现在你可以使用 beeline来测试这个Thrift JDBC/ODBC server:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./bin/beeline
</pre></div>
</div>
<p>在beeline中使用以下命令连接到JDBC/ODBC server：</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>beeline&gt; !connect jdbc:hive2://localhost:10000
</pre></div>
</div>
<p>Beeline会要求你输入用户名和密码。在非安全模式下，只需要输入你本机的用户名和一个空密码即可。对于安全模式，请参考beeline文档中的指示.</p>
<p>将 hive-site.xml，core-site.xml以及hdfs-site.xml文件放置在conf目录下可以完成Hive配置。</p>
<p>你也可以使用Hive 自带的 beeline 的脚本。</p>
<p>Thrift JDBC server还支持通过HTTP传输来发送Thrift RPC消息。使用下面的设置来启用HTTP模式：</p>
<p>hive.server2.transport.mode - Set this to value: http
hive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001
hive.server2.http.endpoint - HTTP endpoint; default is cliservice</p>
<p>为了测试，下面在HTTP模式中使用beeline连接到JDBC/ODBC server:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode<span class="o">=</span>http<span class="p">;</span>hive.server2.thrift.http.path<span class="o">=</span>&lt;http_endpoint&gt;
</pre></div>
</div>
</div>
<div class="section" id="spark-sql-cli">
<h3>运行 Spark SQL CLI<a class="headerlink" href="#spark-sql-cli" title="永久链接至标题">¶</a></h3>
<p>Spark SQL CLI是一个很方便的工具，它可以在本地模式下运行Hive metastore服务，并且执行从命令行中输入的查询语句。注意：Spark SQL CLI无法与Thrift JDBC server通信。</p>
<p>要启动用Spark SQL CLI, 可以在Spark安装目录运行下面的命令:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./bin/spark-sql
</pre></div>
</div>
<p>将 hive-site.xml，core-site.xml以及hdfs-site.xml文件放置在conf目录下可以完成Hive配置。你可以运行 ./bin/spark-sql –help 来获取所有可用选项的完整列表。</p>
</div>
</div>
<div class="section" id="id16">
<h2>迁移指南<a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h2>
<div class="section" id="spark-sql-2-1-2-2">
<h3>Spark SQL 从 2.1 版本升级到2.2 版本<a class="headerlink" href="#spark-sql-2-1-2-2" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>Spark 2.1.1 introduced a new configuration key: spark.sql.hive.caseSensitiveInferenceMode. It had a default setting of NEVER_INFER, which kept behavior identical to 2.1.0. However, Spark 2.2.0 changes this setting’s default value to INFER_AND_SAVE to restore compatibility with reading Hive metastore tables whose underlying file schema have mixed-case column names. With the INFER_AND_SAVE configuration value, on first access Spark will perform schema inference on any Hive metastore table for which it has not already saved an inferred schema. Note that schema inference can be a very time consuming operation for tables with thousands of partitions. If compatibility with mixed-case column names is not a concern, you can safely set spark.sql.hive.caseSensitiveInferenceMode to NEVER_INFER to avoid the initial overhead of schema inference. Note that with the new default INFER_AND_SAVE setting, the results of the schema inference are saved as a metastore key for future use. Therefore, the initial schema inference occurs only at a table’s first access.</p></li>
</ul>
</div>
<div class="section" id="spark-sql-2-0-2-1">
<h3>Spark SQL 从 2.0 版本升级到 2.1 版本<a class="headerlink" href="#spark-sql-2-0-2-1" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>Datasource tables now store partition metadata in the Hive metastore. This means that Hive DDLs such as ALTER TABLE PARTITION … SET LOCATION are now available for tables created with the Datasource API.</p></li>
</ul>
<p>Legacy datasource tables can be migrated to this format via the MSCK REPAIR TABLE command. Migrating legacy tables is recommended to take advantage of Hive DDL support and improved planning performance.
To determine if a table has been migrated, look for the PartitionProvider: Catalog attribute when issuing DESCRIBE FORMATTED on the table.</p>
<ul class="simple">
<li><p>Changes to INSERT OVERWRITE TABLE … PARTITION … behavior for Datasource tables.</p></li>
</ul>
<p>In prior Spark versions INSERT OVERWRITE overwrote the entire Datasource table, even when given a partition specification. Now only partitions matching the specification are overwritten.
Note that this still differs from the behavior of Hive tables, which is to overwrite only partitions overlapping with newly inserted data.</p>
</div>
<div class="section" id="spark-sql-1-6-2-0">
<h3>Spark SQL 从 1.6 版本升级到 2.0 版本<a class="headerlink" href="#spark-sql-1-6-2-0" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>SparkSession 现在是 Spark 新的切入点, 它替代了老的 SQLContext 和 HiveContext。注意：为了向下兼容, 老的 SQLContext 和 HiveContext 仍然保留。可以从 SparkSession 获取一个新的 catalog 接口- 现有的访问数据库和表的API, 如 listTables, createExternalTable, dropTempView, cacheTable 都被移到该接口。</p></li>
<li><p>Dataset API 和 DataFrame API 进行了统一。在 Scala 中, DataFrame 变成了 Dataset[Row]的一个类型别名, 而Java API使用者必须将 DataFrame 替换成 Dataset&lt;Row&gt;。Dataset 类既提供了强类型转换操作 (如 map, filter 以及 groupByKey) 也提供了非强类型转换操作 (如 select 和 groupBy) 。由于编译期的类型安全不是 Python 和 R 语言的一个特性,  Dataset 的概念并不适用于这些语言的 API。相反, DataFrame 仍然是最基本的编程抽象, 就类似于这些语言中单节点数据帧的概念。</p></li>
<li><p>Dataset 和 DataFrame API 中 unionAll 已经过时并且由 union 替代。</p></li>
<li><p>Dataset 和 DataFrame API 中 explode 已经过时。或者 functions.explode() 可以结合 select 或 flatMap 一起使用。</p></li>
<li><p>Dataset 和 DataFrame API 中 registerTempTable 已经过时并且由 createOrReplaceTempView 替代。</p></li>
</ul>
</div>
<div class="section" id="spark-sql-1-5-1-6">
<h3>Spark SQL 从 1.5 版本升级到 1.6 版本<a class="headerlink" href="#spark-sql-1-5-1-6" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>从 Spark 1.6 版本开始，Thrift server 默认运行于多会话模式下, 这意味着每个JDBC/ODBC连接都有独有一份SQL配置和临时函数注册表的拷贝。尽管如此, 缓存的表仍然可以共享。如果你更喜欢在老的单会话模式中运行Thrift server，只需要将spark.sql.hive.thriftServer.singleSession选项设置为true即可。当然，你也可在spark-defaults.conf文件中添加这个选项，或者通过–conf将其传递给start-thriftserver.sh：</p></li>
</ul>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./sbin/start-thriftserver.sh <span class="se">\</span>
     --conf spark.sql.hive.thriftServer.singleSession<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
     ...
</pre></div>
</div>
<ul class="simple">
<li><p>从 Spark 1.6.1 版本开始, sparkR 中的 withColumn 方法支持向 DataFrame 新增一列 或 替换已有的名称相同的列。</p></li>
<li><p>从 Spark 1.6 版本开始, LongType 转换成 TimestampType 将源值以秒而不是毫秒作为单位处理。做出这个变更是为了的匹配Hive 1.2 版本中从数值类型转换成TimestampType的这个行为以获得更一致的类型。更多细节请参见 SPARK-11724 。</p></li>
</ul>
</div>
<div class="section" id="spark-sql-1-4-1-5">
<h3>Spark SQL 从 1.4 版本升级到 1.5 版本<a class="headerlink" href="#spark-sql-1-4-1-5" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>使用手动管理内存(Tungsten引擎)的执行优化以及用于表达式求值的代码自动生成现在默认是启用的。这些特性可以通过将spark.sql.tungsten.enabled的值设置为false来同时禁用。</p></li>
<li><p>默认不启用 Parquet schema 合并。可以将 spark.sql.parquet.mergeSchema 的值设置为true来重新启用。</p></li>
<li><p>Python 中对于列的字符串分解现在支持使用点号(.)来限定列或访问内嵌值，例如 df[‘table.column.nestedField’]。然而这也意味着如果你的列名包含任何点号(.)的话，你就必须要使用反引号来转义它们(例如：table.`column.with.dots`.nested)。</p></li>
<li><p>默认启用内存中列式存储分区修剪。可以通过设置 spark.sql.inMemoryColumarStorage.partitionPruning 值为false来禁用它。</p></li>
<li><p>不再支持无精度限制的decimal，相反, Spark SQL现在强制限制最大精度为38位。从BigDecimal对象推导schema时会使用（38，18）这个精度。如果在DDL中没有指定精度，则默认使用精度Decimal(10，0)。</p></li>
<li><p>存储的时间戳(Timestamp)现在精确到1us（微秒），而不是1ns（纳秒）</p></li>
<li><p>在 sql 方言中，浮点数现在被解析成decimal。HiveQL 的解析保持不变。</p></li>
<li><p>SQL/DataFrame函数的规范名称均为小写(例如：sum vs SUM)。</p></li>
<li><p>JSON数据源不会再自动地加载其他应用程序创建的新文件（例如，不是由Spark SQL插入到dataset中的文件）。对于一个JSON持久化表（例如：存储在Hive metastore中的表的元数据），用户可以使用 REFRESH TABLE 这个SQL命令或者 HiveContext 的 refreshTable 方法来把新文件添加进表。对于一个表示JSON数据集的DataFrame, 用户需要重建这个 DataFrame, 这样新的 DataFrame 就会包含新的文件。</p></li>
<li><p>pySpark 中的 DataFrame.withColumn 方法支持新增一列或是替换名称相同列。</p></li>
</ul>
</div>
<div class="section" id="spark-sql-1-3-1-4">
<h3>Spark SQL 从 1.3 版本升级到 1.4 版本<a class="headerlink" href="#spark-sql-1-3-1-4" title="永久链接至标题">¶</a></h3>
<div class="section" id="id17">
<h4>DataFrame数据读写接口<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h4>
<p>根据用户的反馈，我们提供了一个用于数据读入（SQLContext.read）和数据写出（DataFrame.write）的新的、更加流畅的API，同时老的API（如：SQLCOntext.parquetFile, SQLContext.jsonFile）将被废弃。</p>
<p>有关 SQLContext.read ( Scala, Java, Python ) 和 DataFrame.write ( Scala, Java, Python ) 的更多信息，请参考API文档。</p>
</div>
<div class="section" id="dataframe-groupby">
<h4>DataFrame.groupBy保留分组的列<a class="headerlink" href="#dataframe-groupby" title="永久链接至标题">¶</a></h4>
<p>根据用户的反馈，我们改变了DataFrame.groupBy().agg()的默认行为，就是在返回的DataFrame结果中保留分组的列。如果你想保持1.3版本中的行为，可以将spark.sql.retainGroupColumns设置为false。</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;department&quot;</span><span class="o">,</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">),</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="na">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">);</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">func</span>

<span class="c1"># In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1"># it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;department&quot;</span><span class="p">],</span> <span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># Revert to 1.3.x behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.retainGroupColumns&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="behavior-change-on-dataframe-withcolumn">
<h4>Behavior change on DataFrame.withColumn<a class="headerlink" href="#behavior-change-on-dataframe-withcolumn" title="永久链接至标题">¶</a></h4>
<p>1.4版本之前, DataFrame.withColumn() 只支持新增一列。在DataFrame结果中指定名称的列总是作为一个新列添加进来，即使已经存在了相同名称的列。从1.4版本开始, DataFrame.withColumn() 支持新增一个和现有列名不重复的新列和替换有相同名称的列。</p>
<p>注意：这个变更只针对 Scala API, 不针对 PySpark 和 SparkR。</p>
</div>
</div>
<div class="section" id="spark-sql-1-0-1-2-1-3">
<h3>Spark SQL 从 1.0-1.2 版本升级到 1.3 版本<a class="headerlink" href="#spark-sql-1-0-1-2-1-3" title="永久链接至标题">¶</a></h3>
<p>Spark 1.3版本我们去掉了Spark SQL的 ”Alpha“ 标签并且作为其中的一部分我们在现有的API上做了清理。从Spark 1.3版本开始，Spark SQL将提供1.x系列中其它发行版本的二进制兼容。这个兼容性保证不包括显式地标注为不稳定（例如：DeveloperAPI 或 Experimental）的API。</p>
<div class="section" id="schemardddataframe">
<h4>SchemaRDD重命名为DataFrame<a class="headerlink" href="#schemardddataframe" title="永久链接至标题">¶</a></h4>
<p>升级到Spark SQL 1.3后，用户将会注意到最大的改动就是 SchemaRDD 改名为 DataFrame。主要原因是DataFrame不再直接继承于RDD，而是通过自己的实现来提供RDD中提供的绝大多数功能。通过调用 .rdd 方法 DataFrame 仍然可以转换成RDD。</p>
<p>在Scala中有一个从SchemaRDD到DataFrame的类型别名来提供某些使用场景下的代码兼容性。但仍然建议用户在代码中改用DataFrame。Java和Python用户必须要修改代码。</p>
</div>
<div class="section" id="javascala-api">
<h4>统一Java和Scala API<a class="headerlink" href="#javascala-api" title="永久链接至标题">¶</a></h4>
<p>Spark 1.3 之前的版本中有两个单独的Java兼容类（JavaSQLContext 和 JavaSchemaRDD）可以映射到 Scala API。Spark 1.3版本将Java API和Scala API进行了统一。两种语言的用户都应该使用SQLContext和DataFrame。通常情况下这些类都会使用两种语言中都支持的类型（例如：使用Array来取代语言特有的集合）。有些情况下没有通用的类型（例如：闭包或maps中用于传值），则会使用函数重载。</p>
<p>另外，移除了Java特有的类型API。Scala 和 Java 用户都应该使用 org.apache.spark.sql.types 包中的类来编程式地描述 schema。</p>
</div>
<div class="section" id="dsl-scala">
<h4>隔离隐式转换并删除dsl包(仅针对Scala)<a class="headerlink" href="#dsl-scala" title="永久链接至标题">¶</a></h4>
<p>Spark 1.3版本之前的很多示例代码都以 import sqlContext._ 语句作为开头，这样会引入sqlContext的所有函数。在Spark 1.3版本中我们隔离了RDD到DataFrame的隐式转换，将其单独放到SQLContext内部的一个对象中。用户现在应该这样写：import sqlContext.implicits._。</p>
<p>另外，隐式转换现在也只能使用toDF方法来增加由Product（例如：case classes 或 元祖）组成的RDD，而不是自动转换。</p>
<p>使用 DSL（现在被DataFrame API取代）的内部方法时，用户需要引入 import org.apache.spark.sql.catalyst.dsl。而现在应该要使用公用的  DataFrame函数API：import org.apache.spark.sql.functions._</p>
</div>
<div class="section" id="org-apache-spark-sqldatatype-scala">
<h4>移除org.apache.spark.sql中DataType的类型别名(仅针对Scala)<a class="headerlink" href="#org-apache-spark-sqldatatype-scala" title="永久链接至标题">¶</a></h4>
<p>Spark 1.3版本删除了基础sql包中DataType的类型别名。开发人员应该引入 org.apache.spark.sql.types 中的类。</p>
</div>
<div class="section" id="udfsqlcontext-udf-java-scala">
<h4>UDF注册迁移到sqlContext.udf中(Java&amp;Scala)<a class="headerlink" href="#udfsqlcontext-udf-java-scala" title="永久链接至标题">¶</a></h4>
<p>用于注册UDF的函数，不管是DataFrame DSL还是SQL中用到的，都被迁移到SQLContext中的udf对象中。</p>
<p><strong>Scala</strong></p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">())</span>
</pre></div>
</div>
<p><strong>Java</strong></p>
<div class="highlight-Java notranslate"><div class="highlight"><pre><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="na">udf</span><span class="o">().</span><span class="na">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">(),</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">IntegerType</span><span class="o">);</span>
</pre></div>
</div>
<p>Python UDF注册保持不变。</p>
</div>
<div class="section" id="pythondatatype">
<h4>Python的DataType不再是单例的<a class="headerlink" href="#pythondatatype" title="永久链接至标题">¶</a></h4>
<p>在 Python 中使用DataTypes时，你需要先构造它们（如：StringType()），而不是引用一个单例对象。</p>
</div>
</div>
<div class="section" id="apache-hive">
<h3>兼容Apache Hive<a class="headerlink" href="#apache-hive" title="永久链接至标题">¶</a></h3>
<p>Spark SQL 在设计时就考虑到了和 Hive metastore，SerDes 以及 UDF 之间的兼容性。目前 Hive SerDes 和 UDF 都是基于Hive 1.2.1版本，并且Spark SQL可以连接到不同版本的Hive metastore（从0.12.0到1.2.1，可以参考[与不同版本的Hive Metastore交互]）</p>
<div class="section" id="hive">
<h4>在已有的Hive仓库中部署<a class="headerlink" href="#hive" title="永久链接至标题">¶</a></h4>
<p>Spark SQL Thrift JDBC server采用了开箱即用的设计以兼容已有的 Hive 安装版本。你不需要修改现有的Hive Metastore ,  或者改变数据的位置和表的分区。</p>
</div>
<div class="section" id="id18">
<h4>支持的 Hive 功能<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h4>
<p>Spark SQL 支持绝大部分的Hive功能，如：</p>
<ul class="simple">
<li><dl class="simple">
<dt>Hive查询语句, 包括：</dt><dd><ul>
<li><p>SELECT</p></li>
<li><p>GROUP BY</p></li>
<li><p>ORDER BY</p></li>
<li><p>CLUSTER BY</p></li>
<li><p>SORT BY</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>所有的Hive运算符， 包括：</dt><dd><ul>
<li><p>关系运算符 (=, ⇔, ==, &lt;&gt;, &lt;, &gt;, &gt;=, &lt;=, etc)</p></li>
<li><p>算术运算符 (+, -, <a href="#id19"><span class="problematic" id="id20">*</span></a>, /, %, etc)</p></li>
<li><p>逻辑运算符 (AND, &amp;&amp;, OR, ||, etc)</p></li>
<li><p>复杂类型构造器</p></li>
<li><p>数学函数 (sign, ln, cos等)</p></li>
<li><p>String 函数 (instr, length, printf等)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>用户自定义函数（UDF）</p></li>
<li><p>用户自定义聚合函数（UDAF）</p></li>
<li><p>用户自定义序列化格式（SerDes）</p></li>
<li><p>窗口函数</p></li>
<li><dl class="simple">
<dt>Joins</dt><dd><ul>
<li><p>JOIN</p></li>
<li><p>{LEFT|RIGHT|FULL} OUTER JOIN</p></li>
<li><p>LEFT SEMI JOIN</p></li>
<li><p>CROSS JOIN</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Unions</p></li>
<li><dl class="simple">
<dt>子查询</dt><dd><ul>
<li><p>SELECT col FROM ( SELECT a + b AS col from t1) t2</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>采样</p></li>
<li><p>Explain</p></li>
<li><p>分区表，包括动态分区插入</p></li>
<li><p>视图</p></li>
<li><dl class="simple">
<dt>所有Hive DDL功能, 包括：</dt><dd><ul>
<li><p>CREATE TABLE</p></li>
<li><p>CREATE TABLE AS SELECT</p></li>
<li><p>ALTER TABLE</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>绝大多数 Hive 数据类型，包括：</dt><dd><ul>
<li><p>TINYINT</p></li>
<li><p>SMALLINT</p></li>
<li><p>INT</p></li>
<li><p>BIGINT</p></li>
<li><p>BOOLEAN</p></li>
<li><p>FLOAT</p></li>
<li><p>DOUBLE</p></li>
<li><p>STRING</p></li>
<li><p>BINARY</p></li>
<li><p>TIMESTAMP</p></li>
<li><p>DATE</p></li>
<li><p>ARRAY&lt;&gt;</p></li>
<li><p>MAP&lt;&gt;</p></li>
<li><p>STRUCT&lt;&gt;</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id21">
<h4>不支持的 Hive 功能<a class="headerlink" href="#id21" title="永久链接至标题">¶</a></h4>
<p>以下是目前还不支持的 Hive 功能列表。在 Hive 部署中这些功能大部分都用不到。</p>
<div class="section" id="id22">
<h5>Hive 核心功能<a class="headerlink" href="#id22" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li><p>bucket：bucket是 Hive 表分区内的一个哈希分区，Spark SQL 目前还不支持 bucket。</p></li>
</ul>
</div>
<div class="section" id="id23">
<h5>Hive 高级功能<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li><p>UNION 类型</p></li>
<li><p>Unique join</p></li>
<li><p>列统计数据收集：Spark SQL 目前不依赖扫描来收集列统计数据并且仅支持填充Hive metastore 的 sizeInBytes 字段。</p></li>
</ul>
</div>
<div class="section" id="id24">
<h5>Hive输入输出格式<a class="headerlink" href="#id24" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li><p>CLI文件格式：对于回显到CLI中的结果，Spark SQL 仅支持 TextOutputFormat。</p></li>
<li><p>Hadoop archive</p></li>
</ul>
</div>
<div class="section" id="id25">
<h5>Hive优化<a class="headerlink" href="#id25" title="永久链接至标题">¶</a></h5>
<p>有少数Hive优化还没有包含在Spark中。其中一些（比如索引）由于Spark SQL的这种内存计算模型而显得不那么重要。另外一些在Spark SQL未来的版本中会持续跟踪。</p>
<ul class="simple">
<li><p>块级别位图索引和虚拟列（用来建索引）</p></li>
<li><p>自动为 join 和 groupBy 计算 reducer 个数：目前在 Spark SQL 中，你需要使用 ”SET spark.sql.shuffle.partitions=[num_tasks];” 来控制后置混洗的并行程度。</p></li>
<li><p>仅查询元数据：对于只需要使用元数据的查询请求，Spark SQL 仍需要启动任务来计算结果</p></li>
<li><p>数据倾斜标志：Spark SQL 不遵循 Hive 中的数据倾斜标志</p></li>
<li><p>STREAMTABLE join操作提示：Spark SQL 不遵循 STREAMTABLE 提示。</p></li>
<li><p>对于查询结果合并多个小文件：如果返回的结果有很多小文件，Hive有个选项设置，来合并小文件，以避免超过HDFS的文件数额度限制。Spark SQL不支持这个。</p></li>
</ul>
</div>
</div>
</div>
</div>
<div class="section" id="id26">
<h2>参考<a class="headerlink" href="#id26" title="永久链接至标题">¶</a></h2>
<div class="section" id="id27">
<h3>数据类型<a class="headerlink" href="#id27" title="永久链接至标题">¶</a></h3>
<p>Spark SQL 和 DataFrame 支持以下数据类型：</p>
<ul class="simple">
<li><dl class="simple">
<dt>数值类型</dt><dd><ul>
<li><p>ByteType: 表示1字节长的有符号整型，数值范围：-128 到 127。</p></li>
<li><p>ShortType: 表示2字节长的有符号整型，数值范围：-32768 到 32767。</p></li>
<li><p>IntegerType: 表示4字节长的有符号整型，数值范围：-2147483648 到 2147483647。</p></li>
<li><p>LongType: 表示8字节长的有符号整型，数值范围： -9223372036854775808 to 9223372036854775807。</p></li>
<li><p>FloatType: 表示4字节长的单精度浮点数。</p></li>
<li><p>DoubleType: 表示8字节长的双精度浮点数。</p></li>
<li><p>DecimalType: 表示任意精度的有符号的十进制数。内部使用 java.math.BigDecimal 实现。一个 BigDecimal 由一个任意精度的整数非标度值和一个32位的整数标度组成。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>字符串类型</dt><dd><ul>
<li><p>StringType: 表示字符串值。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>二进制类型</dt><dd><ul>
<li><p>BinaryType: 表示字节序列值。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>布尔类型</dt><dd><ul>
<li><p>BooleanType: 表示布尔值。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>时间类型</dt><dd><ul>
<li><p>TimestampType: 表示由年、月、日、时、分以及秒等字段值组成的时间值。</p></li>
<li><p>DateType: 表示由年、月、日字段值组成的日期值。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>复杂类型</dt><dd><ul>
<li><p>ArrayType(elementType, containsNull)：表示由元素类型为 elementType 的序列组成的值，containsNull 用来标识 ArrayType 中的元素值能否为 null。</p></li>
<li><p>MapType(keyType, valueType, valueContainsNull)：表示由一组键值对组成的值。键的数据类型由 keyType 表示，值的数据类型由 valueType 表示。对于 MapType 值，键值不允许为 null。valueContainsNull 用来表示一个 MapType 的值是否能为 null。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>StructType(fields)：表示由 StructField 序列描述的结构。</dt><dd><ul>
<li><p>StructField(name, datatype, nullable): 表示 StructType 中的一个字段，name 表示字段名。dataType 表示字段的数据类型，nullable 用来表示该字段的值是否可以为 null。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><strong>Scala</strong></p>
<p>Spark SQL 所有的数据类型都位于 org.apache.spark.sql.types 包中。你可以使用下面的语句访问他们:</p>
<div class="highlight-Scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.types._</span>
</pre></div>
</div>
<p>完整示例代码参见 Spark 源码仓库中的 “examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala” 文件。</p>
<p><strong>Java</strong></p>
<p>Spark SQL 所有的数据类型都位于 org.apache.spark.sql.types 包中。如果想要访问或创建一个数据类型, 请使用 org.apache.spark.sql.types.DataTypes 中提供的工厂方法。</p>
<p><strong>Python</strong></p>
<p>Spark SQL 所有的数据类型都位于 pyspark.sql.types 包中。你可以使用下面的语句访问他们:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
<p><strong>R</strong></p>
</div>
<div class="section" id="nan">
<h3>NaN 语义<a class="headerlink" href="#nan" title="永久链接至标题">¶</a></h3>
<p>当处理一些不符合标准浮点语义的 float 或 double 类型时，会对 Not-a-Number(NaN) 做一些特殊处理。具体如下：</p>
<ul class="simple">
<li><p>NaN = NaN 返回true。</p></li>
<li><p>在聚合操作中，所有 NaN 值都被分到同一组。</p></li>
<li><p>在连接键中 NaN 被当做普通值。</p></li>
<li><p>NaN 值按升序排序时排最后，比其他任何数值都大。</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="structured-streaming-guide.html" class="btn btn-neutral float-right" title="Structured Streaming编程指南" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rdd-guide.html" class="btn btn-neutral float-left" title="RDD 编程指南" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, jackiehff

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>